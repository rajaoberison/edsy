[
["index.html", "EDS Vignettes Prerequisites", " EDS Vignettes Andry Rajaoberison 2020-06-29 Prerequisites The following tutorials are written in R. To install R on your computer, please visit this link: https://www.r-project.org/ You should also install RStudio: https://rstudio.com/products/rstudio/download/ We will use the packages in the tidyverse library: install.packages(&quot;tidyverse&quot;) We will use a sample datasets from the Yale Program on Climate Change Communication. The advantage of these data is that they were collected and processed by our colleagues. So we have a good documentation on the sampling methods and the data processing workflow applied to eah of the datasets. "],
["orientation.html", "Section 1 Orientation 1.1 Downloading R-Studio 1.2 Your Work Space 1.3 Helpful Tools for Learning More", " Section 1 Orientation If you have never used R-Studio before, this is the page to start on. We will explain the environment, interface, and tools that make R one of the best platforms for data analysis. If you already have some familiarity with the software or have run programs in R before, feel free to skip this page and instead find the tab with content that answers the specific questions pertaining to your current project. The goal of our resource pages is to provide an introduction to a few key elements of R in the context of data cleaning, visualization, and analysis. Step by step we will provide the reader with a set of tools that allow for introductory to intermediate R usage, providing resources along the way for those interested to dive deeper. 1.1 Downloading R-Studio R is a community sourced, public, free to use tool. To install R-Studio, simply go to this website, select the free RStudio Desktop option, and pick the right option for your computer’s operating system. Once the application is downloaded, open it up so that you can explore the tools along with this guide. 1.2 Your Work Space To test out a little bit of R, we’ve provided a snippet of code below, so you can get a sense of what is happening when you create a new file and what the different outputs looks like. To create a new script, go to File -&gt; New File -&gt; R Script, which should open a blank document in the top left panel. Each numbered line represents a new chunk of logic. They can be run one at a time or collectively as a batch. If you cannot see all four panels in roughly equal dimensions, adjust the margins so that they are all visible. Copy and paste the following code into that top left panel, then click Ctrl+Alt+R to run every line of code together. x &lt;- rnorm(n=100, mean=5, sd=10) mean(x) ## [1] 4.722762 sd(x) ## [1] 10.66027 y &lt;- rnorm(n=100, mean=-3, sd=4) mean(y) ## [1] -2.795328 sd(y) ## [1] 4.3248 plot(x, y, main = &quot;Sample Plot&quot;) What we just did was create two vectors, x and y, which were populated with some randomly generated data. After creating each, we looked at the mean and standard deviation. The random numbers were generated from two distinct normal distributions. We then produced a very simple plot to graphically show the data. Now you should have a frame that looks like the one pictured above. Notice that there are four panels. We will talk through what each of them is used for and what information it provides. The top left panel, where you just put your text, is a representation of the open script that you are working on. This is the place to save any permanent, replicable work that you produce. Much like a Word Document, it is where you keep the final product of your work session. The bottom left window is the Console, which is a temporary workspace scratch pad. Here you can see the lines of code you’ve run and the output from any command that prompts the computer to print something back to you. You can also use the console to try out other lines of code without saving them into a document, and see the output of your commands. For example, go down to the arrow right now in Console, type ‘head(x)’, and then hit enter. You’ll see the result of this command, the first six entries of the vector ‘x’ which we just created. Every time you open up R, you start what is called a session. Anything typed in the Console is stored locally within the current session, but once you restart R, it will disappear. For this reason, any line of code that you want to have access to every time you open a document should be typed and saved in the document you are working on. The top right shows your current global environment. This is where all of the variables, values, and dataframes that you’ve generated can be found. Right now, you should only have only two values shown here, x and y. Each of them is a numerical vector with 100 entries, all generated from the respective normal distributions. We can tell what dimension things are by looking at the index next to the name. When you create a dataframe, it gets added under the ‘Data’ header instead of the ‘Values’ header in this window. The only difference is whether the object is one-dimensional or two-dimensional. Finally, the bottom right will show us several different panels, but right now it should be displaying ‘Plots’. Here we can see the result of our plot() call, with the x-values on the horizontal axis and the y-values on the vertical axis. If you click on the ‘Files’ tab, you can see which folder you are currently working in, otherwise known as your Working Directory (we will discuss what this means in greater detail in the next chapter). For the most part, you’ll want to do your work in the top left panel, so that you can save a record of the calculations that you run. The Console should only be used for temporary or exploratory commands that you don’t want to keep in the permanent document, such as examinations of data or test calculations. If anything goes wrong or if you want a fresh start because you did something that you don’t know how to undo, simply go to Session/Restart R to begin a new session. 1.3 Helpful Tools for Learning More If you ever encounter a function that you don’t know how to use, simply go to the Console and type in ‘?’ followed by the name of the function (ex: ?plot) you want to learn about. RStudio Up will come a helpful blurb describing the purpose the function executes, the usage, and every argument that can be passed to it. ‘Arguments’ in this context are modifiers that tell R how to execute a specific function. The example we gave was for the plot() function, which is used to plot two sets of data against each other. We gave this function the argument ‘main = “Sample Plot”’, which told R to produce a main title for the plot and have the text for that title be “Sample Plot”. Details about what arguments will be accepted by each function can all be found with this ‘?’ query method. Additionally, if you are ever having trouble figuring out what the working directory currently is, you can either type ‘getwd()’ into your Console, or go to the Files tab in the bottom right, click the ‘More’ key next to the image of the gear, and select ‘Go To Working Directory’ to see the folder your script is operating out of. Working directories are especially important when reading in files or accessing data off of your computer. If you ever want to examine a file manually, you can simple call the View() function, which will open up the object as a document in the top left window. This can be useful when looking at the structure of simple datasets or vectors. However, data can often be too large and unwieldy to make an examination like this productive, so you should get comfortable with using summary functions to do this part of your work. Keep in mind that when you have several tabs open, they all will share whatever your current Working Directory is, which can be confusing. If two scripts have names for variables that are identical, this can lead to a plethora of issues, so it is good habit to only be working on one project at a time and to completely restart R before moving on to a different project. "],
["reading.html", "Section 2 Reading 2.1 Explaining Working Directories 2.2 Loading from Package Library 2.3 CSV 2.4 Excel 2.5 Google Spreadsheets 2.6 SPSS 2.7 RDA", " Section 2 Reading In this section, we learn how to read data from different file types. Data can come in many formats, but under the hood most file types operate very similarly. Whether it be a csv (comma separated value), an Excel sheet, a Google sheet, or an SPSS file, all are primarily constructed by rows and columns of entries. Data in each cell is stored as a distinct type of data, such as integer, character, or factor. Reading these files in allows you to explore, manipulate, and eventually return them in a different form using R. 2.1 Explaining Working Directories When you open a document from a folder, R automatically assumes that the folder is your ‘Working Directory’. Think of this like your home base from which you can navigate to any file on your computer. If you want to access a file from the same folder as where you stored your R script (which is the easiest case), all you have to do to make the commands below run successfully is enter the name of the file in quotation marks. Navigating to another folder takes more work. If you have a folder with the file you want to read in nested within the current Working Directory folder, simply type ’NameofFolder + / + NameofFile“. (e.g. ”SubFolder/Datafile.csv“). If you want to back out of your current folder one level, you need to type ‘.. + / + NameofFile’ to move back one layer (e.g. ”../Datafile.csv\"). Moving in two layers or out two layers follows the same pattern, just iterated We highly recommend never manually resetting your working directory, which can be an untraceable and confusing command. We additionally recommend keeping all necessary files stored in intuitive and easy locations to access. 2.2 Loading from Package Library Packages are sets of functions that can be written and published by anybody who uses R. They are typically built around usage themes and can be an incredible resource for specific projects. Because R is a community-based language that has been used in dozens of diverse fields, hundreds of packages exist that are specifically tailored to simplify processes. Before using a new package and reading it from your library, you must first install it onto your computer. All packages are free and typically well documented, so that you can learn quickly what exactly their functions accomplish. An example of installing and loading ‘tidyverse’ can be seen here. You only need to make the ‘install.packages’ call once on your computer. Make sure you are connected to the internet when you do it, since R accesses online resources whenever it installs a new package. After that, all you need to do in a new R document is make a library() call and reference the package you’ll be using. install.packages(&#39;tidyverse&#39;) library(tidyverse) 2.3 CSV CSV files are composed of a row with column headers followed by data entries, where each cell is separated by , and each row starts on a new line. To extract data from a file like this, use the below call, replacing ‘/path/to/csv’ with the local path on your computer to access data using the syntax described above. These parameters written in the code below are also the default values for the read.csv function. The left side of the expression is what the new name of your data will be, and the arrow in the middle means we are assigning the results of the read call on the right to that name. All code lines within the parentheses are the arguments that we pass to the ‘read.csv’ call, which clarify specifically how we want the action to be carried out. Here are the default arguments we mentioned: data &lt;- read.csv(file = &#39;/path/to/csv&#39;, header = TRUE, sep = &#39;,&#39;) Now we will show how to use this call to get data stored somewhere on your computer. Below is an example of reading in a file called ‘hurricane.csv’ from the folder ‘data’. Because all of the default arguments match with what we want, we do not need to write them out explicitly. From now on, in code blocks, we will also be using comments to help the reader track what is happening. A comment is a line of code that we tell the computer not to process as logic but rather to just record. You can write a comment by putting a ‘#’ character at the beginning of the line. All of the text after that on the same line will be treated as a comment. Writing in comments is a good habit, so that you or anybody else using your code can quickly understand what is happening. # Any line of code starting with &#39;#&#39; is a comment and will not be read # Example data &lt;- read.csv(file = &#39;data/hurricane.csv&#39;) Some exceptions: if your file has entries separated by something other than a comma, you can set the ‘sep’ argument to something more specific. Say, for instance, someone has stored data separated by “/”. All you need to do is write sep = ‘/’ as an argument and R will take care of the rest. If you want to control the conversion of character variables to factors, you can pass the argument ‘as.is = TRUE’ to prevent this encoding. 2.4 Excel The main advantage of Excel files is that they can store multiple sheets, while a csv file represents only a single table of data. As such, you must clarify which sheet you are reading from when you make this call. For this example, we’re going to use the readxl package from the comprehensive tidyverse collection, a suite of tools in R. Please visit this website to learn more about tidyverse. To read an excel file, you can use the read_excel function and specify the path/to/the/file and sheet you want to open. If you don’t specify the sheet, read_excel will by default open the first sheet in the spreadsheet. In the ‘eds.excel.sample.xlsx’ file, there are 2 tables: heatwave and hurricane. Here’s how we load both tables into R. Note that this will only work if you make sure that you have downloaded the readxl package using the process we described above: library(readxl) # Reading Sheet 1 and Sheet 2 heatwave &lt;- read_excel(path=&#39;data/excel-sample.xlsx&#39;, sheet = &#39;heatwave&#39;) hurricane &lt;- read_excel(path=&#39;data/excel-sample.xlsx&#39;, sheet = &#39;hurricane&#39;) Once the tables are stored in individual R variables, you can perform exploration and analysis on them. 2.5 Google Spreadsheets If the data is stored in a Google spreadsheet, we can read it using the googledrive and googlesheet4 packages. We use the googledrive package to log into our Google Drive account and googlesheets4 to read the speadsheets in our drive. In the example below, we used a spreadsheet named eds.sample.googlesheets which contains the same tables as in the previous Excel example (heatwave and hurricane). You can clone the spreadsheet via this link if you’d like to follow along and repeat the steps below using your own Google account. Authenticate to your drive using drive_auth(). When prompted, log in, authorize GoogleDrive, and use the authorization code if provided. You only need to run drive_auth() once. library(googledrive) # To authenticate and authorize googledrive package drive_auth() The following scripts show how to explore a Google Drive folder. This is not recommended, as you might encounter performance issues. # NOT recommended # To view the list of files in a folder drive_ls(&quot;EDS&quot;) # where &quot;EDS&quot; is the folder name # To also get the files within the subfolders drive_ls(&quot;EDS&quot;, recursive = TRUE) # To view the list of spreadsheets within a folder drive_ls(&quot;EDS&quot;, type=&quot;spreadsheet&quot;) Also, because of Google’s authentification system, you may run into an error like we do below when running the previous code (using drive_ls()). Glitches like this are why it is not recommended. #&gt; Error in add_id_path(nodes, root_id = root_id, leaf = leaf) : !anyDuplicated(nodes$id) is not TRUE To avoid this, you can use the folder url instead of the folder name. The folder url can be obtained by right-clicking on the folder and selecting Get shareable link. Then run the following code: # If using the folder name doesn&#39;t work folder_url = &#39;https://drive.google.com/open?id=1e0uJ9dwFcL34JA61F0tGSoaiMZ_xio_4&#39; drive_ls(folder_url, type=&quot;spreadsheet&quot;) Then you can load the spreadsheet by using its id eds.sample.spreadsheet &lt;- drive_get(id = &#39;1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc&#39;) It is also possible to read the spreadsheet right away by using its link/path (without using drive_ls()). We recommend using this to read any Google Drive files. eds.sample.spreadsheet &lt;- drive_get(path = &#39;https://drive.google.com/open?id=1uIsgrcsevbm9voZU-rzqhTg2LE5SgEPlGabSXKTcQtc&#39;) Once the spreadsheet is loaded, we run a similar code used for the Excel files to read tables within the spreadsheet. But for Google Sheets, this function is called read_sheet: library(googlesheets4) # Authorizing the googlesheets4 package sheets_auth(token=drive_token()) # Reading the tables heatwave &lt;- read_sheet(eds.sample.spreadsheet, sheet = &#39;heatwave&#39;) hurricane &lt;- read_sheet(eds.sample.spreadsheet, sheet = &#39;hurricane&#39;) If this presents too much of a headache and you’d rather download the Google Spreadsheet as either a csv or Excel file, you can save it locally to your computer, put it in the same folder as your R scripts, and use one of the two methods described above to read the file into your R environment. 2.6 SPSS SPSS is a statistical software package that stores data in its own format. You will occasionally encounter these types of files if working on a project with somebody who prefers SPSS. This requires its own package called ‘haven’. library(haven) data &lt;- read_sav(&quot;data/spss-sample.sav&quot;) By default, the read_sav() will read the factor levels of non-numeric and non-character variables. If instead we want the labels, we can run the following code: library(magrittr) library(dplyr) # Applying haven::as_factor() to labelled columns Here, we already know that # variables Zone, Q4 and Q50 are not factor variables. data %&gt;% mutate_at(vars(-Zone, -Q4, -Q50), as_factor) ## # A tibble: 1,130 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 2 3 0 Moderately Pr… No 1928 Male $70,000-$99… ## 2 A 1 4 0 Moderately Pr… No 1962 Male &lt;NA&gt; ## 3 A 3 4 0 Moderately Pr… No 1931 Fema… Over $200,0… ## 4 A 3 6 1 Fully Prepared No 1950 Male $100,000-$1… ## 5 A 2 Not Worried … 0 Very Prepared No 1948 Male $100,000-$1… ## 6 A 5 4 0 Very Prepared No 1938 Fema… &lt;NA&gt; ## 7 A 3 6 1 Moderately Pr… No 1977 Fema… &lt;NA&gt; ## 8 A 5 4 0 Moderately Pr… No 1964 Fema… &lt;NA&gt; ## 9 A 1 3 0 Moderately Pr… No 1976 Male $40,000-$69… ## 10 A 2 6 0 Very Prepared No 1964 Fema… Over $200,0… ## # … with 1,120 more rows Because variables can be labelled in SPSS, we can use them as well to find what each column represents. # To get the labels of the variables / columns as.vector(unlist(lapply(data, function(x) attributes(x)$label))) ## [1] &quot;Q4. Since the beginning of 2009, how many hurricanes and tropical storms, if any, hit your city or town on or near the Connecticut coast while you were at home; that is, not out of town?&quot; ## [2] &quot;Q5. Generally speaking, when a hurricane or tropical storm is approaching your city or town, how worried do you feel? Please answer using the following scale ranging from 1 (not at all worried) to 7 (extremely worried).&quot; ## [3] &quot;Q6. Since the beginning of 2009, how many times, if ever, did you leave your home for someplace safer to avoid a hurricane or tropical storm; that is, how many times did you evacuate? Please enter the number in the boxes below.&quot; ## [4] &quot;Q7. Generally speaking, how prepared were you for the storm(s) you experienced?&quot; ## [5] &quot;Q10. Before Superstorm Sandy hit your area, did you leave your home to go someplace safer; that is, did you evacuate?&quot; ## [6] &quot;Q50. In what year were you born?&quot; ## [7] &quot;Q51. Are you...?&quot; ## [8] &quot;Q59. Last year (in 2013), what was your total HOUSEHOLD income from all sources?&quot; To learn more about the haven package and how the variables are stored, please visit: https://haven.tidyverse.org/ 2.7 RDA Other times you will encounter a native RDA file, which is a way of saving R data structures such as vectors, matrices, and data frames in an automatically compressed form. The benefit to an RDA file is that it loads or saves slightly quicker in R software than other file types, which can matter when you are dealing with massive files. Reading these files in is incredibly simple, using the following command: load(file = &quot;mydata.rda&quot;) # If you want to save your current work as an rda save(x, file = &quot;myRDA.rda&quot;) Just as you did for the last several procedures, replace ‘mydata.rda’ with the path to the file you are trying to read in. Because this is a native file type, no package is required. "],
["extraction.html", "Section 3 Extraction 3.1 Web Scraping", " Section 3 Extraction 3.1 Web Scraping Web scraping is the process of fetching and extracting information or data from a webpage. It is very useful if you want to create a dynamic database that updates itself based on the content of a specific website without having to return and manually update the data repeatedly. To scrape a webpage, we first need to know how to get to the webpage. It can be helpful to retrieve a url that you can use to directly access the content. For example, to obtain the Google search results for “data science”, you can copy and paste this url to your browser: https://www.google.com/search?q=data+science . Using this url allows you to access results without having to type “data science” on a Google search web page. While this example is rather trivial, website scraping is a critical tool for more complicated or extensive data pages. Some websites like Twitter or Facebook will require to you to use an API for authentification in order to access their data. API stands for Application Programming Interface. It is a computing interface which connects interactions between intermediaries and gives the user a set of conventions to follow for the extraction process. For this example, we are going to use The Weather Channel’s website, which does not require authentification. We will extract the 10-day forecast for a specific location and store the data in a dataframe in R for later usage. After inspecting the website and its url, you will notice that you can view the weather data by zip code using this url pattern: https://weather.com/weather/ + forecast type + /l/ + zip_code + :4:US If we want to view the 10-day forecast for New Haven, we can go to: https://weather.com/weather/tenday/l/06511:4:US. And if we want to access data for today’s forecast: https://weather.com/weather/today/l/06511:4:US Once we have the webpage url, we can read it into R and extract the data using rvest from the tidyverse collection. We will use the same process of loading from library to get this working. If you are confused by what was just said or if this is not working for you, return to the previous chapter on reading in documents to acquaint yourself with package loading in R. The New Haven 10-day forecast webpage looks like this: weatherpage What we want now is a table that has comprehensive coverage of the weather information for our selected criteria and time window. In order to extract these values, we have to know where in the source code they are located. For example, in the “DAY” column, we want to extract the exact date instead of the days of the week. And we can do that by: Inspecting the tag or class of exact date from the website. Moving the cursor to the exact date, right-clicking, then choosing Inspect When a window opens, it will point directly to the location of the exact date in the source code. Take notes of the css (tag or class name) and use it to get the exact date value using the html_nodes() function. Since this is complicated, we highly recommend following along with this example before trying to apply these steps to your own procedure. Look at the below image to understand what we mean: weatherpage Here is how we extract the dates: # Load your necessary library library(rvest) ## Loading required package: xml2 # Get the webpage url and store it for access url = &#39;https://weather.com/weather/tenday/l/06511:4:US&#39; # Load the webpage using the url webpage &lt;- read_html(url) # Getting the exact date ####################################################### # Filtering the relevant css / location # This line comes from our inspection of the webpage as described above date_locations &lt;- html_nodes(webpage, &quot;span.day-detail.clearfix&quot;) # Extracting the exact value raw_date &lt;- html_text(date_locations) print(raw_date) ## character(0) # Because the value are formatted like &quot;NOV 21&quot; we have to convert to a date format exact_date &lt;- as.Date(raw_date, format=&quot;%b %d&quot;) # b = month, d = day print(exact_date) ## Date of length 0 You will see in the code above that we used a function called as.Date() and gave an argument ‘format= \"%b %d’ to it. This means we want to reformat the date to have month listed first, and day listed second. Here is the full code that extracts the complete table: library(rvest) # Get the webpage url url = &#39;https://weather.com/weather/tenday/l/06511:4:US&#39; # Load the webpage using the url webpage &lt;- read_html(url) # Getting the exact date ####################################################### # Filtering the relevant css / location date_locations &lt;- html_nodes(webpage, &quot;span.day-detail.clearfix&quot;) # Extracting the exact value raw_date &lt;- html_text(date_locations) # Because the value are formatted like &quot;Nov 21&quot; we have to convert to a date format exact_date &lt;- as.Date(raw_date, format=&quot;%b %d&quot;) # b = month, d = day # Getting the weather description desc_loc &lt;- html_nodes(webpage, &quot;td.description&quot;) desc &lt;- html_text(desc_loc) # Getting the temperature temp_loc &lt;- html_nodes(webpage, &quot;td.temp&quot;) temp &lt;- html_text(temp_loc) # High and Low temperature values high_temp &lt;- rep(NA, length(temp)) low_temp &lt;- rep(NA, length(temp)) for (i in 1:length(temp)){ all &lt;- unlist(strsplit(temp[i], &quot;°&quot;)) if (length(all) &gt; 1){ high_temp[i] &lt;- all[1] low_temp[i] &lt;- all[2] } else { low_temp[i] &lt;- 38 } } # Getting the precipitation precip_loc &lt;- html_nodes(webpage, &quot;td.precip&quot;) precip &lt;- as.numeric(sub(&quot;%&quot;, &quot;&quot;, html_text(precip_loc))) / 100 # Getting the wind wind_loc &lt;- html_nodes(webpage, &quot;td.wind&quot;) wind &lt;- html_text(wind_loc) # Wind direction and speed wind_dir &lt;- rep(NA, length(wind)) wind_speed &lt;- rep(NA, length(wind)) for (i in 1:length(wind)){ all &lt;- unlist(strsplit(wind[i], &quot; &quot;)) wind_dir[i] &lt;- all[1] wind_speed[i] &lt;- all[2] } # Getting the humidity humidity_loc &lt;- html_nodes(webpage, &quot;td.humidity&quot;) humidity &lt;- as.numeric(sub(&quot;%&quot;, &quot;&quot;, html_text(humidity_loc))) / 100 # Save the data in tibble library(tibble) new_haven_forecast &lt;- tibble(&#39;day&#39; = exact_date, &#39;description&#39; = desc, &#39;high_temp&#39; = high_temp, &#39;low_temp&#39; = low_temp, &#39;precip&#39; = precip, &#39;wind_dir&#39; = wind_dir, &#39;wind_speed&#39; = wind_speed, &#39;himidity&#39; = humidity) new_haven_forecast ## # A tibble: 0 x 8 ## # … with 8 variables: day &lt;date&gt;, description &lt;chr&gt;, high_temp &lt;lgl&gt;, ## # low_temp &lt;dbl&gt;, precip &lt;dbl&gt;, wind_dir &lt;chr&gt;, wind_speed &lt;chr&gt;, ## # himidity &lt;dbl&gt; This can take some trial and error, so do not be dissuaded if your first attempt at extraction does not work out exactly as you had anticipated. Sometimes the computer will read in the lines that you want, but not in the format that you need them in, at which point you’ll need to directly manipulate the data. To learn some common ways of doing that, go to the next chapter on Structuring Data. "],
["structuring.html", "Section 4 Structuring 4.1 Accessing Data in R 4.2 Data types 4.3 Inspecting the data 4.4 Subsetting and Filtering 4.5 Changing cell values 4.6 Pivoting the dataset 4.7 Merging Two Dataframes", " Section 4 Structuring Data structuring is the process of correcting, rearranging, or removing inaccurate records from raw data so that, after the treatment, the transformed data will be easy to analyze. The variable names, types, and values must be consistent and uniform. The focus here is on the ‘appearance’ of the data. We often restructure data so that we can merge two dataframes with pairable attributes, combine several sources describing the same event, or prepare data for intense statistical analysis that requires consistent and complete data. Whatever the purpose, having clean data is a good habit regardless of application or functionality. If somebody else picks up your project, you do not want to have to explain to them the complications and intricacies of messy data. The first stage of structuring is to make sure that your read-in process gave you data of the type that you anticipated or intended. Before we can do that, we should learn how to access specific parts of the data for examination. 4.1 Accessing Data in R Once you have read in your data, you may want to look, change, or perform calculations on a specific part or element. Keep in mind that R uses 1-indexing instead of 0-indexing, which other languages like Python employ. 1-indexing means that you start counting from the number 1 instead of 0. To access a range of values in a dataframe, you can specify a set of numbers or column names. Let’s create a dataframe below so that you can get a sense of how this works. x &lt;- data.frame(replicate(5, sample(1:10,20,rep=TRUE))) names(x) &lt;- c(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;) What we’ve just done above is create a dataframe with 5 columns, each of which has 20 rows populated by a random number between 1 and 10. The names of these columns are ‘One’, ‘Two’, ‘Three’, ‘Four’, and ‘Five’ respectively. Let’s say that we want to access just the first three rows of the third and fourth columns. We can do that one of two ways: # Accessing a part of the data x[1:3, 3:4] x[1:3, c(&quot;Three&quot;, &quot;Four&quot;)] The first line shows how we use indexing to access data. The rows are listed first, then after a comma the columns are listed. If we want a range of values, we use ‘:’ to indicate we want every number between the two values. 1:4 means we want rows 1, 2, 3, and 4 while just typing 4 means we want just row 4. We use the same logic for columns. However, if we want to access columns by name, we can generate a vector within c(), which is the R shorthand way of writing a vector. Each value within c() is treated as a single variable, separated by commas. In the example, we typed c(“Three”, “Four”) to get the third and fourth columns. Whatever the names of the columns that you want are, you can use this method to access them. You can also use c() to get a non-consecutive set of numbers. Say we want the first, third, and fifth column. We could use c(1,3,5) to get just these columns. Let’s say we just want one column from a dataframe to use for examination or manipulation. We can use the ‘$’ character to clarify which one we want by name. You will see this all over R scripts, as it is very common to want to access a particular column or item in a list in its entirety. # Accessing just the fifth column x$Five x$Five[6:8] The first command gives us just the fifth column of the frame. The second command gives us the sixth to eighth row of the fifth column, using the same square bracket [] indexing as before. Typically, square brackets are reserved for indexing, and parentheses are used for functions. One final method of subsetting is throuch exclusion. You can type ‘-’ in front of a number if you want to access everything but a single column. For example, instead of typing c(1,2,3,5) to get all columns except for the fourth, we can write ‘-4’. # These are equivalent statements x[,c(1,2,3,5)] x[,-4] 4.2 Data types One type of anomaly that we may encounter is the coercion of irrelevant data types to variables. This is very common for numerically coded variables, or variables that have distinct levels. First we read in our data by the method we practiced in the earlier chapter: If we read in the same SPSS data from the Reading data section, we get the coded values instead of the labels. ## # A tibble: 6 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+l&gt; &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; ## 1 A 2 3 0 3 [Moderate… 2 [No] 1928 1 [Mal… 4 [$70,000… ## 2 A 1 4 0 3 [Moderate… 2 [No] 1962 1 [Mal… NA ## 3 A 3 4 0 3 [Moderate… 2 [No] 1931 2 [Fem… 6 [Over $2… ## 4 A 3 6 1 1 [Fully Pr… 2 [No] 1950 1 [Mal… 5 [$100,00… ## 5 A 2 1 [Not Worr… 0 2 [Very Pre… 2 [No] 1948 1 [Mal… 5 [$100,00… ## 6 A 5 4 0 2 [Very Pre… 2 [No] 1938 2 [Fem… NA It appears as though some of the categorical data was read in as integers. This is a quirk of R, that if left unchanged can drastically change the outcome of your analyses. To access the specific type of data of one column, we can use the class() function, then pass the name of the frame and column we are trying to access using the technique outlined above: class(data$Q6) ## [1] &quot;numeric&quot; Q6 is meant to be categorical, not numeric. If we run summary(data), we get this unintended result which affects several columns: ## Warning in mask$eval_all_mutate(dots[[i]]): NAs introduced by coercion ## Zone Q4 Q5 Q6 ## Min. : NA Min. : 0.000 Min. :1.000 Min. :0.0000 ## 1st Qu.: NA 1st Qu.: 2.000 1st Qu.:1.000 1st Qu.:0.0000 ## Median : NA Median : 2.000 Median :2.000 Median :0.0000 ## Mean :NaN Mean : 2.537 Mean :1.617 Mean :0.4191 ## 3rd Qu.: NA 3rd Qu.: 3.000 3rd Qu.:2.000 3rd Qu.:1.0000 ## Max. : NA Max. :20.000 Max. :2.000 Max. :4.0000 ## NA&#39;s :1130 NA&#39;s :134 NA&#39;s :968 NA&#39;s :116 ## Q7 Q10 Q50 Q51 Q59 ## Min. :1.000 Min. :1.000 Min. : 19 Min. :1.00 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:2.000 1st Qu.:1944 1st Qu.:1.00 1st Qu.:3.000 ## Median :3.000 Median :2.000 Median :1955 Median :2.00 Median :4.000 ## Mean :2.674 Mean :1.796 Mean :1944 Mean :1.55 Mean :3.715 ## 3rd Qu.:3.000 3rd Qu.:2.000 3rd Qu.:1966 3rd Qu.:2.00 3rd Qu.:5.000 ## Max. :5.000 Max. :2.000 Max. :1992 Max. :2.00 Max. :6.000 ## NA&#39;s :114 NA&#39;s :123 NA&#39;s :47 NA&#39;s :38 NA&#39;s :112 Q4 and Q50 are the only variables that are supposed to be numeric, but here everything is treated as numeric, even when it was originally categorical data, which is incorrect. We might also want to treat Zone as a factor so that we can examine groups collectively based on this metric. We can easily convert data types into factors using either dplyr::mutate_at() and applying as.factor function to the variables or the as.[] series of commands in BaseR. Examples of both are given below: # Converting data types updated_data &lt;- data %&gt;% mutate_at(vars(-Q4, -Q6, -Q50), as_factor) # Alternately, for specific columns data$Q6 &lt;- as.factor(data$Q6) And now we can get the full summary statistics in the form that we want: ## Zone Q4 Q5 Q6 ## A:684 Min. : 0.000 5 :244 Min. :0.0000 ## B:446 1st Qu.: 2.000 4 :211 1st Qu.:0.0000 ## Median : 2.000 3 :169 Median :0.0000 ## Mean : 2.537 6 :129 Mean :0.4191 ## 3rd Qu.: 3.000 2 :104 3rd Qu.:1.0000 ## Max. :20.000 (Other):162 Max. :4.0000 ## NA&#39;s :134 NA&#39;s :111 NA&#39;s :116 ## Q7 Q10 Q50 Q51 ## Fully Prepared : 93 Yes :205 Min. : 19 Male :491 ## Very Prepared :326 No :802 1st Qu.:1944 Female:601 ## Moderately Prepared:438 NA&#39;s:123 Median :1955 NA&#39;s : 38 ## A Little Prepared :137 Mean :1944 ## Not at all Prepared: 22 3rd Qu.:1966 ## NA&#39;s :114 Max. :1992 ## NA&#39;s :47 ## Q59 ## Less than $15,000: 81 ## $15,000-$39,999 :169 ## $40,000-$69,999 :215 ## $70,000-$99,999 :190 ## $100,000-$199,999:220 ## Over $200,000 :143 ## NA&#39;s :112 As we can see from the summary, there might still be anomalies with the variables: Q4: Number of storms experienced: The mean value is 2.5 but the max value is 20, which suggests heavily right skewed data. Q50: Birth year: Some respondent answered 19 which is incorrect. It’s pretty clearly a typo, because nobody is 2,001 years old. We may also consider transforming this column into age instead of birth year, which makes it easier to interpret. Perpetually in data analysis, the issue of missing values and how to deal with them comes up. We can see here that this dataset is no different, with several NA values across the board. We will talk at length about missing data later on in the next chapter on Data Cleaning. 4.3 Inspecting the data In order to structure a dataset, we need to not only detect the anomalies within the data, but also decide what to do with them. Such anomalies can include values that are stored in the wrong format (ex: a number stored as a string), values that fall outside of the expected range (ex: outliers), values with inconsistent patterns (ex: dates stored as mm/dd/year vs dd/mm/year), trailing spaces in strings (ex: “data” vs “data”), or other logistical issues. With our data successfully read in above, we can examine both structure and summary statistics for numerical and categorical variables. # Structure of the data str(data[,1:3]) ## tibble [1,130 × 3] (S3: tbl_df/tbl/data.frame) ## $ Zone: chr [1:1130] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;A9&quot; ## ..- attr(*, &quot;display_width&quot;)= int 1 ## $ Q4 : num [1:1130] 2 1 3 3 2 5 3 5 1 2 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Q4. Since the beginning of 2009, how many hurricanes and tropical storms, if any, hit your city or town on or n&quot;| __truncated__ ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 2 ## $ Q5 : dbl+lbl [1:1130] 3, 4, 4, 6, 1, 4, 6, 4, 3, 6, 7, 5, 7, ... ## ..@ label : chr &quot;Q5. Generally speaking, when a hurricane or tropical storm is approaching your city or town, how worried do you&quot;| __truncated__ ## ..@ format.spss: chr &quot;F1.0&quot; ## ..@ labels : Named num [1:2] 1 7 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Not Worried At All&quot; &quot;Extremely Worried&quot; # Summary for a numerical variable summary(data$Q4) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 2.000 2.000 2.537 3.000 20.000 134 # Summary for a categorical variable summary(as_factor(data$Q7)) ## Fully Prepared Very Prepared Moderately Prepared A Little Prepared ## 93 326 438 137 ## Not at all Prepared NA&#39;s ## 22 114 Here we will look at the output of a few other key functions. Calling head() will by default show you the first six rows of a dataframe. If you want to see more or fewer rows, simply put a comma in after the dataframe’s name and write the number of rows you want to display. Calling tail() will give you the exact opposite, displaying the last n rows of data, with the default again set to six. # First 10 rows head(data, 10) ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;fct&gt; &lt;dbl+lbl&gt; &lt;dbl+l&gt; &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; ## 1 A 2 3 0 3 [Moderate… 2 [No] 1928 1 [Mal… 4 [$70,00… ## 2 A 1 4 0 3 [Moderate… 2 [No] 1962 1 [Mal… NA ## 3 A 3 4 0 3 [Moderate… 2 [No] 1931 2 [Fem… 6 [Over $… ## 4 A 3 6 1 1 [Fully Pr… 2 [No] 1950 1 [Mal… 5 [$100,0… ## 5 A 2 1 [Not Worr… 0 2 [Very Pre… 2 [No] 1948 1 [Mal… 5 [$100,0… ## 6 A 5 4 0 2 [Very Pre… 2 [No] 1938 2 [Fem… NA ## 7 A 3 6 1 3 [Moderate… 2 [No] 1977 2 [Fem… NA ## 8 A 5 4 0 3 [Moderate… 2 [No] 1964 2 [Fem… NA ## 9 A 1 3 0 3 [Moderate… 2 [No] 1976 1 [Mal… 3 [$40,00… ## 10 A 2 6 0 2 [Very Pre… 2 [No] 1964 2 [Fem… 6 [Over $… # Last 10 rows tail(data, 10) ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl+l&gt; &lt;fct&gt; &lt;dbl+lbl&gt; &lt;dbl+lb&gt; &lt;dbl&gt; &lt;dbl+lb&gt; &lt;dbl+lbl&gt; ## 1 B 1 2 0 4 [A Little P… 2 [No] 1980 1 [Male] 3 [$40,000-… ## 2 B 2 2 0 3 [Moderately… 2 [No] 1977 2 [Fema… 4 [$70,000-… ## 3 B 4 4 1 2 [Very Prepa… 1 [Yes] 1962 2 [Fema… 2 [$15,000-… ## 4 B 2 5 0 1 [Fully Prep… 2 [No] 1946 1 [Male] 5 [$100,000… ## 5 B NA 4 &lt;NA&gt; 1 [Fully Prep… 1 [Yes] 1957 2 [Fema… 1 [Less tha… ## 6 B 1 4 1 4 [A Little P… 1 [Yes] 1987 2 [Fema… 6 [Over $20… ## 7 B 2 5 0 3 [Moderately… 2 [No] 1953 1 [Male] 4 [$70,000-… ## 8 B NA 4 4 2 [Very Prepa… 2 [No] 1973 2 [Fema… 1 [Less tha… ## 9 B 2 5 0 3 [Moderately… 2 [No] 1980 1 [Male] 5 [$100,000… ## 10 B 2 2 0 4 [A Little P… 2 [No] NA 2 [Fema… 3 [$40,000-… To find the number of rows and columns your dataframe has, call dim(). To find just the rows, use nrow(). To find just the columns, use ncol(). Calling either names() or colnames() will give you the names of the columns. If your data has row names (which is not always the case), you can use rownames() to access those. # Dataframe dimensions dim(data) ## [1] 1130 9 # Total number of rows nrow(data) ## [1] 1130 # Total number of columns ncol(data) ## [1] 9 # Column names names(data) # also colnames(data) ## [1] &quot;Zone&quot; &quot;Q4&quot; &quot;Q5&quot; &quot;Q6&quot; &quot;Q7&quot; &quot;Q10&quot; &quot;Q50&quot; &quot;Q51&quot; &quot;Q59&quot; We can also plot the data to visualize the distribution of variables using the dplyr and magrittr # Plotting the first 5 columns plot(data[,1:5]) If you use these techniques and see an outlier, whether it be an abnormally large or small number, you can use either the min() or max() function to get the specific value. Because we have missing values, we need to pass the argument ‘na.rm = TRUE’ to ignore all NA values when calculating these. For several numerical evaluations, R makes sure to explicitly highlight the presence of NA values, which are treated as unknowns. Because R does not know if the missing value could have been the minimum or maximum, it will tell you that this calculation is NA unless you explicitly give it permission to ignore these missing values. # Examine the problematic values we identified earlier max(data$Q4, na.rm = TRUE) ## [1] 20 min(data$Q50, na.rm = TRUE) ## [1] 19 4.4 Subsetting and Filtering We can remove incorrect or missing row values by using dplyr::filter: # Removing rows where birth year is irrelevant # Here we decided that all birth year must be greater 1900 updated_data &lt;- data %&gt;% filter(Q50 &gt; 1900) # Now if we re-run its summary we get the following summary(updated_data$Q50) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1908 1945 1955 1956 1966 1992 # Removing rows with birth year greater than 1900 and missing responses for Q4 updated_data &lt;- data %&gt;% filter(Q50 &gt; 1900, !is.na(Q4)) summary(updated_data$Q50) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1908 1944 1954 1955 1965 1990 summary(updated_data$Q4) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 2.000 2.000 2.522 3.000 20.000 We can also select for only the variables that we are interested in using the function dplyr::select: # Creating a new dataframe with only zone, gender, and income columns updated_data &lt;- data %&gt;% select(Zone, Q59, Q51) head(updated_data, 10) ## # A tibble: 10 x 3 ## Zone Q59 Q51 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 4 1 ## 2 A &lt;NA&gt; 1 ## 3 A 6 2 ## 4 A 5 1 ## 5 A 5 1 ## 6 A &lt;NA&gt; 2 ## 7 A &lt;NA&gt; 2 ## 8 A &lt;NA&gt; 2 ## 9 A 3 1 ## 10 A 6 2 plot(table(updated_data), las=1) It is also possible to split the dataset into multiple dataframes by number of rows using split(): # To split the dataset into multiple dataframes of 10 rows each max_number_of_rows_per_dataframe &lt;- 10 total_number_rows_in_the_current_dataset &lt;- nrow(data) sets_of_10rows_dataframes &lt;- split(data, rep(1:ceiling(total_number_rows_in_the_current_dataset/max_number_of_rows_per_dataframe), each=max_number_of_rows_per_dataframe, length.out=total_number_rows_in_the_current_dataset) ) # Here are the first 2 dataframes sets_of_10rows_dataframes[[1]] # or sets_of_10rows_dataframes$`1` ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 2 3 0 3 2 1928 1 4 ## 2 A 1 4 0 3 2 1962 1 &lt;NA&gt; ## 3 A 3 4 0 3 2 1931 2 6 ## 4 A 3 6 1 1 2 1950 1 5 ## 5 A 2 1 0 2 2 1948 1 5 ## 6 A 5 4 0 2 2 1938 2 &lt;NA&gt; ## 7 A 3 6 1 3 2 1977 2 &lt;NA&gt; ## 8 A 5 4 0 3 2 1964 2 &lt;NA&gt; ## 9 A 1 3 0 3 2 1976 1 3 ## 10 A 2 6 0 2 2 1964 2 6 sets_of_10rows_dataframes[[2]] ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 2 7 2 1 1 1937 2 &lt;NA&gt; ## 2 A 3 5 0 2 2 1943 1 4 ## 3 A 2 7 0 2 2 1954 2 5 ## 4 A 2 5 0 2 2 1959 2 5 ## 5 A 4 1 &lt;NA&gt; 2 2 1936 2 6 ## 6 A 1 3 1 3 1 1963 1 6 ## 7 A 2 3 1 2 1 1950 2 5 ## 8 A 4 6 0 3 2 NA &lt;NA&gt; &lt;NA&gt; ## 9 A 0 4 0 2 2 1941 1 5 ## 10 A NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1952 2 5 4.5 Changing cell values As we mentioned earlier, it is best if Q50 is stored as an age variable instead of the default birth year. Q50 is a numeric variable and we can simply change it by using dplyr::mutate() With the filtered data, we can replace all of the values in Q50 with 2020 - Q50 like so: # Replacing Q50 values to their age in 2020 updated_data &lt;- data %&gt;% mutate(Q50 = 2020 - Q50) head(updated_data, 10) ## # A tibble: 10 x 9 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 2 3 0 3 2 92 1 4 ## 2 A 1 4 0 3 2 58 1 &lt;NA&gt; ## 3 A 3 4 0 3 2 89 2 6 ## 4 A 3 6 1 1 2 70 1 5 ## 5 A 2 1 0 2 2 72 1 5 ## 6 A 5 4 0 2 2 82 2 &lt;NA&gt; ## 7 A 3 6 1 3 2 43 2 &lt;NA&gt; ## 8 A 5 4 0 3 2 56 2 &lt;NA&gt; ## 9 A 1 3 0 3 2 44 1 3 ## 10 A 2 6 0 2 2 56 2 6 # It is also possible to leave Q50 untouched and store the results into a new column updated_data &lt;- data %&gt;% mutate(age = 2020 - Q50) head(updated_data, 10) ## # A tibble: 10 x 10 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 age ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 2 3 0 3 2 1928 1 4 92 ## 2 A 1 4 0 3 2 1962 1 &lt;NA&gt; 58 ## 3 A 3 4 0 3 2 1931 2 6 89 ## 4 A 3 6 1 1 2 1950 1 5 70 ## 5 A 2 1 0 2 2 1948 1 5 72 ## 6 A 5 4 0 2 2 1938 2 &lt;NA&gt; 82 ## 7 A 3 6 1 3 2 1977 2 &lt;NA&gt; 43 ## 8 A 5 4 0 3 2 1964 2 &lt;NA&gt; 56 ## 9 A 1 3 0 3 2 1976 1 3 44 ## 10 A 2 6 0 2 2 1964 2 6 56 summary(updated_data$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 28.00 54.00 65.00 64.14 75.00 112.00 For a categorical variable, we use a different function: dplyr::recode_factor() or dplyr::recode(). We will apply this to Q5 as we have noticed in the previous section that not all of its values were labelled from SPSS. Here is its summary: ## 1 2 3 4 5 6 7 NA&#39;s ## 58 101 164 197 232 123 97 104 Looking back at the questionnare, here is how it was phrased: Because the survey itself does not have labels, the recoding will be up to the user. Here we chose to replace the extreme values with 1 and 7. As mentioned in the documentation: dplyr::recode() will preserve the existing order of levels while changing the values, and dplyr::recode_factor() will change the order of levels to match the order of replacements. # Recoding Q5 recoded.with.recode &lt;- recode(data$Q5, `Not Worried At All`=&quot;1&quot;, `Extremely Worried`=&quot;7&quot;) summary(recoded.with.recode) ## 1 2 3 4 5 6 7 NA&#39;s ## 58 101 164 197 232 123 97 104 recoded.with.recode_factor &lt;- recode_factor(data$Q5, `Not Worried At All`=&quot;1&quot;, `Extremely Worried`=&quot;7&quot;) summary(recoded.with.recode_factor) ## 1 7 2 3 4 5 6 NA&#39;s ## 58 97 101 164 197 232 123 104 We can also change cell values without external libraries like dplyr by running the following code: # Add column age where the values are 2020 - Q50 data$age &lt;- 2020 - data$Q50 # Replace Q5 with value &quot;Not Worried At All&quot; to &quot;1&quot; data$Q5[data$Q5 == &quot;Not Worried At All&quot;] &lt;- 1 Notice above that we used a conditional subset in the second command. Just like we used indexing earlier, we can also use a Boolean vector (True/False) to pick only rows where some condition is met. Within the [] brackets, we gave a statement: ‘data$Q5 == \"Not Worried At All’ using ‘==’ to indicate we want True/False for each row. This syntax will return only the entries of Q5 that are True within the condition. We then reassign the value for each of these to be ‘1’ instead of ‘Not Worried At All’, effectively doing the same task as we did above. R is smart enough to work with vectors in this way, running the same evaluation on each entry without you specifying. One final set of functions that can be incredibly helpful with changing cell values are gsub() and grep(). If, for instance, you want to find all entries of a column that have a specific string pattern in them, you can use grep() to locate every instance. Simply type in the pattern, and the frame over which you want the function to look. For instance, if we wanted to find every entry of a column with the word “missing” in it, we could use grep() like so: # Identify every row that contains the phrase &quot;missing&quot; grep(&quot;missing&quot;, x$example) Now let’s say that instead of simply locating every instance, we instead want to replace them with a substitute phrase. That is when we would use gsub(), which lets us substitute another phrase in. If we wanted to replace every part of every entry that says “CT” with “Connecticut”, we could do so: # Replace every instance of &#39;CT&#39; with &#39;Connecticut&#39; gsub(&quot;CT&quot;, &quot;Connecticut&quot;, x$statesample) 4.6 Pivoting the dataset In some cases, we may want to split a column based on values, or merge multiple columns into fewer columns. A classic example is with time-stamped data. Sometimes we want rows to be days, with individual responses recorded as columns, and sometimes we want each row to be an individual, with columns representing days. It just depends on what the data are modelling. This process can be done using the tidyr package. For example, to convert the dataframe into long-format with only Zone, question, and value as columns: # Read in the necessary package library(tidyr) # We have to pivot by variable type # Pivot longer for factor variables pivoted.longer &lt;- data %&gt;% select_if(is.factor) %&gt;% pivot_longer(-Zone, names_to = &quot;question&quot;, values_to = &quot;value&quot;) pivoted.longer ## # A tibble: 6,456 x 3 ## Zone question value ## &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; ## 1 A Q5 3 ## 2 A Q6 0 ## 3 A Q7 3 ## 4 A Q10 2 ## 5 A Q51 1 ## 6 A Q59 4 ## 7 A Q5 4 ## 8 A Q6 0 ## 9 A Q7 3 ## 10 A Q10 2 ## # … with 6,446 more rows # Then we can reshape it back to the original pivoted.wider &lt;- pivoted.longer %&gt;% group_by(question) %&gt;% mutate(row = row_number()) %&gt;% pivot_wider(names_from = question, values_from = value) %&gt;% select(-row) pivoted.wider ## # A tibble: 1,076 x 7 ## Zone Q5 Q6 Q7 Q10 Q51 Q59 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 A 3 0 3 2 1 4 ## 2 A 4 0 3 2 1 &lt;NA&gt; ## 3 A 4 0 3 2 2 6 ## 4 A 6 1 1 2 1 5 ## 5 A 1 0 2 2 1 5 ## 6 A 4 0 2 2 2 &lt;NA&gt; ## 7 A 6 1 3 2 2 &lt;NA&gt; ## 8 A 4 0 3 2 2 &lt;NA&gt; ## 9 A 3 0 3 2 1 3 ## 10 A 6 0 2 2 2 6 ## # … with 1,066 more rows tidyr::spread() and tidyr::gather() are the outdated equivalent of tidyr::pivot_wider() and tidyr::pivot_longer(). To merge or split columns, we can use tidyr::unite() or tidyr::separate(). For example, to merge Q7 and Q10: # Creating a new column with responses from both Q7 and Q10 merged &lt;- data %&gt;% unite(&quot;Q7_Q10&quot;, Q7:Q10, sep = &quot;__&quot;, remove = TRUE, na.rm = FALSE) merged ## # A tibble: 1,076 x 9 ## Zone Q4 Q5 Q6 Q7_Q10 Q50 Q51 Q59 age ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 2 3 0 3__2 1928 1 4 92 ## 2 A 1 4 0 3__2 1962 1 &lt;NA&gt; 58 ## 3 A 3 4 0 3__2 1931 2 6 89 ## 4 A 3 6 1 1__2 1950 1 5 70 ## 5 A 2 1 0 2__2 1948 1 5 72 ## 6 A 5 4 0 2__2 1938 2 &lt;NA&gt; 82 ## 7 A 3 6 1 3__2 1977 2 &lt;NA&gt; 43 ## 8 A 5 4 0 3__2 1964 2 &lt;NA&gt; 56 ## 9 A 1 3 0 3__2 1976 1 3 44 ## 10 A 2 6 0 2__2 1964 2 6 56 ## # … with 1,066 more rows # To split it back merged %&gt;% separate(Q7_Q10, c(&quot;Q7&quot;, &quot;Q10&quot;), sep = &quot;__&quot;, remove = TRUE) ## # A tibble: 1,076 x 10 ## Zone Q4 Q5 Q6 Q7 Q10 Q50 Q51 Q59 age ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A 2 3 0 3 2 1928 1 4 92 ## 2 A 1 4 0 3 2 1962 1 &lt;NA&gt; 58 ## 3 A 3 4 0 3 2 1931 2 6 89 ## 4 A 3 6 1 1 2 1950 1 5 70 ## 5 A 2 1 0 2 2 1948 1 5 72 ## 6 A 5 4 0 2 2 1938 2 &lt;NA&gt; 82 ## 7 A 3 6 1 3 2 1977 2 &lt;NA&gt; 43 ## 8 A 5 4 0 3 2 1964 2 &lt;NA&gt; 56 ## 9 A 1 3 0 3 2 1976 1 3 44 ## 10 A 2 6 0 2 2 1964 2 6 56 ## # … with 1,066 more rows 4.7 Merging Two Dataframes If we want to combine two dataframes based on a shared column, we can do that with a ‘merge’ call. Let’s say we have two separate dataframes that discuss the same individuals. Each participant was given an ID that was recorded over the two frames, and we want one dataframe with their responses joined. # Here we will create the simulated data described above survey1 &lt;- cbind(1:10, data.frame(replicate(10,sample(0:1,1000,rep=TRUE)))) survey2 &lt;- cbind(1:10, data.frame(replicate(10,sample(0:1,1000,rep=TRUE)))) # Name the columns of both frames names(survey1) &lt;- c(&quot;ID&quot;, paste0(&quot;Q&quot;, 1:10)) names(survey2) &lt;- c(&quot;ID&quot;, paste0(&quot;Q&quot;, 11:20)) The first column of each dataframe represents the participant’s ID, and the next 10 columns represent their responses to questions. We can use a ‘merge’ to execute this: # Merge the two by their &#39;ID&#39; column surveytotal &lt;- merge(survey1, survey2, by = &quot;ID&quot;) With the merge command, we can do one of four types: an inner join, a left join, a right join, or a full join. With an inner join, we only include data that have an identification match in both sets. A left join saves all information from the left dataset and puts NA values in if there is not a corresponding row in the right, a right join does the opposite, preserving all of the right frame’s data, and a full join saves all rows from both of the original datasets. The frame surveytotal now has all questions stored in the same place. If the “ID” column does not match up in format, this can give you issues, so make sure you only merge once your data is fully cleaned, which we will talk about in the next chapter. We can also do two simpler combines, known as cbind() or rbind(), if the data do not need to be merged on an ID entry. Say, for example, we had two sheets of an Excel file which were completed on different days and had the exact same set of columns&gt; If we want them to be in the same frame, one after the other, we could use an rbind() call to bind the rows like so: # Generate the top dataframe top &lt;- data.frame(replicate(10, sample(0:1,1000,rep=TRUE))) names(top) &lt;- paste0(&quot;Row_&quot;, 1:10) # Generate the bottom dataframe bottom &lt;- data.frame(replicate(10, sample(0:1,1000,rep=TRUE))) names(bottom) &lt;- paste0(&quot;Row_&quot;, 1:10) # Use rbind() to bring them together combined &lt;- rbind(top, bottom) If there are two frames that have the same number of rows that describe in sequence the same observations, we can use a cbind() call to combine them: # Create the left data left &lt;- data.frame(replicate(10, sample(0:1,1000,rep=TRUE))) names(left) &lt;- paste0(&quot;Row_&quot;, 1:10) # Create the right data right &lt;- data.frame(replicate(10, sample(0:1,1000,rep=TRUE))) names(right) &lt;- paste0(&quot;Row_&quot;, 11:20) # Use cbind() to stick them together combined &lt;- cbind(left, right) Keep in mind that this is only a valid merge if the two frames are composed of rows that represent the same observations. "],
["cleaning.html", "Section 5 Cleaning 5.1 Fixing skewed distribution 5.2 Treating outliers 5.3 Fixing missing values", " Section 5 Cleaning Data cleaning is the process of getting data ready for statistical analysis. In contrast to structuring the data, the target anomalies in this case are variable values such as missing values, outliers, skewed distributions, etc. 5.1 Fixing skewed distribution Data are skewed when their distribution is not symmetrical but rather favors either the left or right. Sometimes, to facilitate statistical analysis, we need to transform skewed data so that it becomes normally distributed instead. Several tests like PCA analysis require multivariate normality. First we will read in data to use for this part of our practice. We will then examine a plot. # Example of skewed data {x &lt;- data$nstorm h &lt;- hist(x, breaks=20, col=&quot;grey&quot;) xfit &lt;- seq(min(x), max(x), length=40) yfit &lt;- dnorm(xfit, mean=mean(x), sd=sd(x)) yfit &lt;- yfit*diff(h$mids[1:2])*length(x) lines(xfit, yfit, col=&quot;red&quot;, lwd=2)} The plot above shows a good example of skewed data. To deal with this type of data, a good first tool to try is some form of logarithmic transformation, which reduces the scale of the larger values more substantially. # Log-transformation {log.x &lt;- log(data$nstorm) h &lt;- hist(log.x, breaks = 10, col = &quot;grey&quot;)} While the previous log-transformation seem to have worked, it is not entirely correct because the variable nstorm has 0 values which cannot be represented in logarithms. In this case, one of the commonly used methods is square root. # sqrt transformation {sqrt.x &lt;- sqrt(data$nstorm) h &lt;- hist(sqrt.x, breaks = 10, col = &quot;grey&quot;)} While these two are not the only types of transformation you can make, they are certainly two of the most frequently used. 5.2 Treating outliers Outliers are extreme values in datasets. If not detected and handled appropriately, they can affect the accuracy of the predicitions and analysis. Non-resilient statistics are those that can be affected by severe outliers, like a mean calculation getting pulled in the direction of extreme values. Resilient statistics are those that can withstand a few outliers, like a median value. Treating outliers depends on a good knowledge of the data and it is up to the data analyst to decide on how to go about on fixing them. Selva Prabhakaran gave some ways of handling outliers that we are going to show below. You can detect outliers using a boxplot where they will show up as dots outside the whiskers: # For continuous variable outlier_values &lt;- unique(boxplot.stats(data$nstorm)$out) # outlier values. boxplot(data$nstorm, main=&quot;How many storm have you experienced?&quot;, boxwex=0.1) mtext(paste(&quot;Outliers: &quot;, paste(outlier_values, collapse=&quot;, &quot;)), cex=0.6) # For categorical variable boxplot(nstorm ~ worry, data=data, main=&quot;Number of storms experienced across levels of worry&quot;) It is worth noting that outliers should not always, if ever, be ignored or removed. While they are anomalies that make modelling difficult, quite often they represent a part of the data’s narrative which should not be swept aside without consideration. The techniques we describe are for outliers which, for whatever reason, do not belong in the dataset, whether that be corrupted data, mislabelled points, or typos. If an outlier must be removed for a condition of normality to be met, it is in the hands of the statistician to explain why such steps were necessary in light of the procedures being run. Approaches for outlier treatment include imputation with mean, median, or mode. We can also cap the values, predict them, or delete the observations. Deletion should only be done in the case that there is clear, undisputable evidence that the data entry was recorded incorrectly. 5.3 Fixing missing values Ways of detectng missing values (NA) include the summary and anyNA(). # Check to see if there are missing values anyNA(data$nstorm) ## [1] TRUE summary(data$nstorm) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 2.00 2.00 2.58 3.00 20.00 50 Sometimes you want to access only the entries that are not missing within a specific column. For these cases, the is.na() function is incredibly useful. It returns a vector of True/False for entries which are missing or not missing. In R, if you want to flip a Boolean (T/F) value, simply use the ‘!’ character. This comes in handy when, for instance, we want only the non-missing values # Test to show ! usage TRUE == !FALSE ## [1] TRUE # Returns only the real values data$nstorm[!is.na(data$nstorm)] ## [1] 2 1 3 3 2 5 3 5 1 2 3 2 2 4 1 4 0 3 2 6 3 3 3 1 2 ## [26] 2 2 2 2 5 2 2 4 2 5 2 5 3 1 8 2 2 3 2 2 2 2 2 2 5 ## [51] 2 2 1 2 2 3 1 2 2 3 2 7 1 2 4 1 4 2 2 2 4 3 2 1 1 ## [76] 2 2 1 2 2 2 3 6 4 5 2 2 3 3 2 2 1 2 2 4 2 4 2 2 3 ## [101] 2 2 2 2 3 2 2 3 1 2 3 2 2 2 3 3 2 2 2 2 2 5 2 2 2 ## [126] 2 4 2 0 1 2 1 5 2 3 5 3 2 2 2 2 4 6 3 2 3 3 1 2 0 ## [151] 2 2 2 1 2 3 2 3 4 2 2 0 1 1 3 12 4 4 2 2 3 5 2 3 1 ## [176] 1 2 4 9 5 2 1 0 1 4 2 4 3 1 1 1 2 2 2 2 2 2 3 2 3 ## [201] 6 3 2 4 3 2 3 3 3 3 1 2 2 3 2 3 4 2 1 2 2 4 0 3 4 ## [226] 2 5 2 4 3 3 5 2 1 2 2 3 2 2 2 3 2 1 2 2 2 2 1 3 2 ## [251] 3 4 3 3 3 2 1 3 2 2 2 11 2 2 2 2 1 2 3 2 2 5 2 2 3 ## [276] 2 2 2 3 2 2 2 10 2 2 3 3 0 2 2 2 3 2 2 0 2 2 3 2 2 ## [301] 3 2 2 3 1 3 3 2 6 7 5 1 2 3 1 1 3 1 2 2 5 2 4 1 1 ## [326] 2 2 2 2 4 6 1 0 4 2 4 2 2 2 2 2 4 2 4 2 3 2 3 2 1 ## [351] 2 2 2 2 1 6 1 2 3 2 1 1 2 2 3 2 3 4 2 2 3 2 2 2 10 ## [376] 2 2 2 2 1 0 2 3 3 3 3 2 3 2 3 3 2 1 3 5 1 2 2 2 2 ## [401] 2 5 3 0 2 2 2 4 1 3 2 3 3 2 4 4 2 2 2 1 3 1 1 5 2 ## [426] 4 1 4 2 1 2 2 2 2 3 2 3 1 1 2 1 3 2 12 5 0 3 5 2 2 ## [451] 1 4 4 0 4 8 2 7 9 1 3 2 2 2 2 1 3 1 2 2 2 3 2 1 0 ## [476] 0 3 2 0 2 4 0 3 1 2 2 2 0 4 3 2 3 10 3 2 2 2 5 10 3 ## [501] 1 1 1 3 2 6 2 2 1 3 3 2 3 2 2 5 3 3 2 2 3 2 3 2 2 ## [526] 4 2 2 3 2 4 3 2 5 2 1 3 1 3 2 1 2 2 4 2 3 3 3 1 1 ## [551] 2 1 3 4 1 2 6 2 6 2 3 0 2 3 4 2 2 2 1 1 2 2 2 4 2 ## [576] 3 5 4 2 10 2 6 1 3 4 2 1 3 2 1 2 2 2 2 3 3 1 2 3 8 ## [601] 3 2 2 2 1 5 1 1 3 1 4 5 2 0 2 2 1 2 1 2 2 2 1 8 1 ## [626] 2 2 5 2 3 2 2 1 3 1 7 4 4 2 2 2 2 2 20 2 1 3 1 0 2 ## [651] 2 3 2 2 3 5 1 0 4 10 4 0 3 5 10 1 3 2 2 2 2 1 3 5 2 ## [676] 4 3 2 1 2 4 2 2 4 2 5 3 2 0 3 2 1 2 4 1 1 1 2 2 3 ## [701] 2 1 5 2 2 2 2 2 2 2 1 2 3 3 2 5 1 3 3 3 2 1 2 1 2 ## [726] 7 5 3 2 2 8 4 2 3 4 2 1 1 4 4 3 2 4 3 2 2 10 1 2 4 ## [751] 5 2 5 2 3 2 2 2 2 1 2 0 2 2 2 2 2 3 5 2 2 3 2 2 0 ## [776] 2 2 2 2 3 3 2 3 2 6 3 3 1 5 3 2 2 2 5 3 2 2 2 0 2 ## [801] 4 3 3 1 2 1 5 3 3 2 2 3 3 4 3 2 2 1 1 6 3 5 1 3 3 ## [826] 1 4 3 2 1 3 2 5 0 2 2 2 4 2 3 1 4 3 4 1 2 6 1 2 2 ## [851] 2 6 3 2 2 3 3 3 1 3 4 3 3 2 1 3 1 2 4 3 1 3 3 1 4 ## [876] 2 2 3 1 1 2 2 2 1 1 6 2 2 2 1 8 2 2 2 3 3 1 1 2 5 ## [901] 4 3 3 2 4 3 0 3 5 2 1 2 4 2 1 2 2 2 The simplest way to handle missing values is by removing all of the corresponding observations using tidyr::drop_na() or, in some cases, removing a variable itself. The goal of treating NA values is to expereince as little data loss as possible while maintaining your overall data integrity. If, for example, a column has only 5 real values and 95 missing values, it’s probably not worth keeping in your dataset. If a row has only 1 real value and 11 missing values, it also might not be worth keeping. The discretion of treatment is largely in the hands of the analyst. The command below is a sweeping way to remove rows with any missing values, which is effective when you have rare missing values and voluminous data to choose from. tidyr::drop_na(data) ## # A tibble: 776 x 13 ## nstorm worry nevac prepared homeloc nyear gender income politics age zone ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2 3 0 3 2 86 1 4 3 86 A ## 2 3 4 0 3 3 19 2 6 2 83 A ## 3 3 6 1 1 1 52 1 5 3 64 A ## 4 2 1 0 2 3 15 1 5 3 66 A ## 5 1 3 0 3 3 4 1 3 1 38 A ## 6 2 6 0 2 2 17 2 6 2 50 A ## 7 3 5 0 2 3 70 1 4 4 71 A ## 8 2 7 0 2 1 24 2 5 2 60 A ## 9 2 5 0 2 2 18 2 5 2 55 A ## 10 1 3 1 3 2 2 1 6 4 51 A ## # … with 766 more rows, and 2 more variables: lat &lt;dbl&gt;, long &lt;dbl&gt; However, if dropping all of the rows with missing values will affect the quality of the data or its breadth, then another option is to replace the missing values with the mean/median/mode of the variable, or to predict for them using an appropriate modelling algorithm. There are several packages out there that are solely dedicated to treating missing values including VIM and MICE. In this next example, we’ll try to predict the 15 missing values in the variable nstorm (number of storms the survey respondents have experienced) using the variables that has no missing values: zone, lat, and long. # Imputation using MICE library(mice) # Building the mice model mice_model &lt;- mice(select(data, zone, lat, long, nstorm), method=&quot;rf&quot;, printFlag=F) ## Warning: Number of logged events: 1 # Predicting the missing values # generate the completed data mice_prediction &lt;- complete(mice_model) # checking for NAs anyNA(mice_prediction) ## [1] FALSE Then we can visualize the data to see how well the imputation has performed. However, the best way to assess the accuracy is to compare actual values with predicted values using measures such as: MSE, MAE, MAPE, etc. # Visualizing the prediction non_na_latitude &lt;- data$lat[!is.na(data$nstorm)] non_na_nstorm &lt;- data$nstorm[!is.na(data$nstorm)] na_latitude &lt;- mice_prediction$lat[is.na(data$nstorm)] na_nstorm &lt;- mice_prediction$nstorm[is.na(data$nstorm)] plot(non_na_nstorm, non_na_latitude, col=&quot;grey&quot;, pch=&quot;•&quot;, ylab=&quot;Latitude&quot;, xlab=&quot;Number of Storms Experienced&quot;) points(na_nstorm, na_latitude, col=&quot;red&quot;, pch=&quot;•&quot;, cex=2) legend(&quot;topright&quot;, c(&quot;Existing values&quot;, &quot;Predicted missing values&quot;), col=c(&quot;grey&quot;, &quot;red&quot;), pch=&quot;•&quot;, cex=1.5) Other ways to impute the missing values are with mean, median, or mode. # Assign the data to prediction prediction &lt;- data na.observations &lt;- is.na(data$nstorm) prediction$nstorm[na.observations] &lt;- median(prediction$nstorm[!na.observations]) # Visualizing the prediction na_latitude &lt;- prediction$lat[na.observations] na_nstorm &lt;- prediction$nstorm[na.observations] plot(non_na_nstorm, non_na_latitude, col=&quot;grey&quot;, pch=&quot;•&quot;, ylab=&quot;Latitude&quot;, xlab=&quot;Number of Storms Experienced&quot;) points(na_nstorm, na_latitude, col=&quot;red&quot;, pch=&quot;•&quot;, cex=2) legend(&quot;topright&quot;, c(&quot;Existing values&quot;, &quot;Predicted missing values&quot;), col=c(&quot;grey&quot;, &quot;red&quot;), pch=&quot;•&quot;, cex=1.5) For mode, there is no built-in function in R, but you can access one here: getmode &lt;- function(v) { uniqv &lt;- unique(v) uniqv[which.max(tabulate(match(v, uniqv)))] } "],
["visualization.html", "Section 6 Visualization 6.1 Simple graphs with ggplot2 6.2 Animated graph with gganimate 6.3 Interactive graph with plotly 6.4 Web app with RShiny 6.5 More data visualization resources", " Section 6 Visualization 6.1 Simple graphs with ggplot2 GGPlot2 is one of the most universally applicable visualization packages currently available for R-Studio. It provides a set of tools that make it incredibly dynamic and versatile. As such, we will provide an introductory examination of some of the best features that you can integrate into your analysis. Data visualization is about telling the story of the data in a digestible way and highlighting key traits that would not otherwise be apparent. Whether you are using plots for personal exploratory analysis or in a final presentation of your findings, they can transform a sheet of numbers into a decipherable and palatable image. # Read in the necessary packages for our work library(haven) library(ggplot2) library(tidyr) library(dplyr) # Read in the sav file, and filter it raw.data &lt;- read_sav(&#39;data/ypccc-hurricane.sav&#39;) raw.data &lt;- raw.data %&gt;% filter(raw.data$Q1==1 || raw.data$Q2==1) raw.data &lt;- raw.data %&gt;% filter(!is.na(raw.data$Q3), raw.data$Q3!=2, raw.data$Q3!=3) # Recode number codes into text labels raw.data$S5_cluster &lt;- recode(as.character(raw.data$S5_cluster), &#39;1&#39;=&#39;DieHards&#39;, &#39;2&#39;=&#39;Reluctant&#39;, &#39;4&#39;=&#39;Optimists&#39;, &#39;3&#39;=&#39;Constrained&#39;, &#39;5&#39;=&#39;First Out&#39;) # Save S5_cluster as a factor raw.data$S5_cluster &lt;- factor(raw.data$S5_cluster, ordered=T, levels=c(&#39;First Out&#39;, &#39;Constrained&#39;, &#39;Optimists&#39;, &#39;Reluctant&#39;, &#39;DieHards&#39;)) # Keep only the non-missing data data &lt;- raw.data[!is.na(raw.data$S5_cluster),] The raw data that we will be using for this section covers Hurricanes and their attached attributes. 6.1.1 Setting ggplot2 theme The ‘theme’ of ggplot2 is the aesthetic with which it produces every default plot or figure. By setting the theme, you are able to customize size, color palette, positioning of labels, font, text characteristics, and a whole series of other qualities which are well documented online. Below, we save a standard theme as ‘plot_theme’ so that we can use it later. Within the description, we clarify margins, text size, key size, and element sizes. To set color, we use HTML color codes, a standard across many coding languages. Color can be described by a six character string, consisting of numbers and characters in hexadecimal form. You can go to this [website] (https://html-color.codes/) to access a color palette selector if there is a particular shade you want. You then pass this code as ‘# + xxxxxx’ to the color = () argument. # Save the plot theme plot_theme &lt;- theme( # Describe margins and sizes legend.title = element_blank(), legend.box.background = element_rect(), legend.box.margin = margin(6, 6, 6, 6), legend.background = element_blank(), legend.text = element_text(size=12), legend.key.size = unit(1, &quot;cm&quot;) ) + theme( # Describe production and color of the axes axis.text = element_text(size=12, face=&#39;bold&#39;), axis.line = element_line(size = 1, colour = &quot;#212F3D&quot;), axis.title.x = element_blank(), axis.title.y = element_blank() ) + theme( # Describe the title and background plot.title = element_text(size = 20, face=&#39;bold&#39;, hjust = 0.5, margin=margin(6,6,6,6)), plot.background=element_blank() ) + theme( # Describe the grid lines panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.grid.major.y = element_line(size=1) ) # To see all information about your current theme, you can use theme_get() description &lt;- theme_get() 6.1.2 ggplot2 Example 1: Stacked bar plot The first example we will provide is that of a stacked bar plot, which shows the statistics of distribution across categorical levels within a separate categorical level. For our example, we will show the percentage of Connecticut residents who understand that their home is in an evacuation zone, split across ‘First Out’, ‘Constrained’, ‘Optimists’, ‘Reluctant’, or ‘DieHards’. Plots like this one are good for getting a sense of how two categorical variables are related, much as you might observe in a two-way table or statistically evaluate with a Chi-square Test for Homogeneity. Take a look at the code below for an example of a call to ggplot. First, we make sure that our data is saved as categorical variables, and that it is sorted and filtered appropriately. Next, we call ggplot() and pass it the dataframe we want to use, which in this case is Q31_Data, a new frame that has the points stored as percentages for each part of the stacked bars. The second argument we pass is aes(), which we can fill with details clarifying the aesthetic we want to use. The ‘x=’ is the data on the x-axis the ‘y=’ is the data on the y-axis. The fill is what distinguishes each segment of the stacked bar. Once we have told ggplot() what data it will be using, we put ‘+’ and clarify the type of plot we want to construct, in this case geom_bar(). Every time we write ‘+’, it means that we are making a further adjustment to the existing plot under construction. We give arguments clarifying the aesthetics further, then stick on geom_text() to describe what labels to use, how to render them, and where to put them on the plot. To set the colors manually, we call scale_fill_manual() and pass the color codes we want used. Calling scale_y_continuous() lets us make the y-axis operate on a 0-100% scale. Using labs() lets us put the label text in manually, and theme() lets us clarify where we want that main title. The results can be seen below. # Q31. Is your home located in a hurricane evacuation zone, or not? data$Q31 &lt;- as_factor(data$Q31) data$Q31 &lt;- factor(data$Q31, ordered=T, levels=c(&#39;No&#39;, &#39;Not Sure&#39;, &#39;Yes&#39;)) # Create the dataframe of just the relevant columns, summarized into sums and taken as a percentage Q31_data &lt;- data %&gt;% select(Q31, S5_cluster, Final_wgt_pop) %&gt;% # na.omit() %&gt;% group_by(S5_cluster, Q31) %&gt;% summarise(Q31.wgt = sum(Final_wgt_pop)) %&gt;% mutate(freq.wgt = round(100*Q31.wgt/sum(Q31.wgt))) %&gt;% filter(!is.na(Q31)) # Produce the plot, using the ggplot call ggplot(Q31_data, aes(x=S5_cluster, y=freq.wgt, fill=Q31)) + geom_bar(aes(fill=Q31), width=.7, stat=&#39;identity&#39;) + geom_text(aes(label=freq.wgt, vjust=.5), position = position_stack(vjust = 0.5), colour=&#39;white&#39;, size=6, fontface=&#39;bold&#39;) + scale_fill_manual(values=c(&#39;#DC7633&#39;, &#39;#5D6D7E&#39;, &#39;#5499C7&#39;), breaks=c(&#39;Yes&#39;, &#39;Not Sure&#39;, &#39;No&#39;)) + scale_y_continuous(labels = function(x) paste0(x, &quot;%&quot;), expand = c(0, 0), limits = c(0, 100)) + labs(title = &quot;% of CT coastal residents who understand\\nthat their home is in an evacuation zone&quot;) + theme(legend.position=&quot;top&quot;) + plot_theme + theme(legend.box.background = element_blank(), legend.spacing.x = unit(.2, &#39;cm&#39;)) While it may seem overwhelming at first to use ggplot2, keep in mind that you can supply as much or as little detail and customization to your plot as you want. What we have done in this document is generate finely tuned plots to highlight a wide variety of customizable tools, but if your goal is to simply put the numbers in a plot, that can be done quite intuitively. 6.1.3 ggplot2 Example 2: Grouped bar plot If, instead of a stacked bar plot, you want to group the bars next to each other in sequence to get a sense not only of percentage but also total count, you can use a grouped bar plot as we have done below. For this example, we use the perception of people within each segment about how likely they think it is that a Category 1, 2, 3, or 4 hurricane will hit somewhere along the Connecticut coast in the next 50 years. # Q23. On a scale of 0%-100%, with 0% being it definitely will NOT happen and 100% being it definitely WILL happen, # how likely do you think it is that each of the following types of hurricane will hit somewhere along the # Connecticut coast in the next 50 years? Q23_data &lt;- data %&gt;% select(S5_cluster, Q23_1, Q23_2, Q23_3, Q23_4, Final_wgt_pop) %&gt;% pivot_longer(-one_of(&#39;S5_cluster&#39;, &#39;Final_wgt_pop&#39;), names_to=&quot;Category&quot;, names_prefix = &quot;Q23_&quot;, values_to = &quot;Q23&quot;) %&gt;% mutate(Category = paste(&quot;Category&quot;, Category), Q23.wgt = Q23 * Final_wgt_pop) %&gt;% na.omit() %&gt;% group_by(S5_cluster, Category) %&gt;% summarise(freq.wgt = round(sum(Q23.wgt, na.rm=T)/sum(Final_wgt_pop, na.rm=T))) # Produce the plot ggplot(Q23_data, aes(x=S5_cluster, y=freq.wgt, fill=Category)) + geom_bar(aes(fill=Category), width=.7, position=position_dodge(.8), stat=&#39;identity&#39;) + geom_text(aes(label=freq.wgt, vjust=1.2), position=position_dodge(.8), colour=&#39;white&#39;, size=6, fontface=&#39;bold&#39;) + scale_fill_manual(values=c(&#39;#F6DDCC&#39;, &#39;#EDBB99&#39;, &#39;#E59866&#39;, &#39;#DC7633&#39;)) + scale_y_continuous(labels = function(x) paste0(x, &quot;%&quot;), expand = c(0, 0), limits = c(0, 100)) + labs(title = &quot;Average perception of each segment that\\na Category 1, 2, 3, or 4 hurricane will occur\\nin the next 50 years&quot;) + theme(legend.position=&quot;top&quot;) + plot_theme + theme(legend.box.background = element_blank(), legend.spacing.x = unit(.2, &#39;cm&#39;)) 6.1.4 ggplot2 Example 3: Scatterplot The plots below are examples of how to make a more complicated scatterplot. The data in the first plot describe state-level average annual temperature anomalies by average annual precipitation. In addition to the simple two-way scatterplot, we can also color the points by a third categorical variable, in this case the period of time in which the data were recorded. All labels can be customized as we do below, and we’ve added error bars to show uncertainty in a shaded gray region around the data. We’ve commented out a line of code that can be used to save your plot as a .png file, which is the default image file type in R. You can customize the saved file’s name, height, and width within a call like this. Whenever we ask R to save a dataset, a file, a plot, or an image, it will assume that we want it in our current Working Directory. If, instead, you want to store the output somewhere else, you can clarify the path when you write out the name. The second, simpler plot that we generated below displays California temperature anomalies by year. We are going to put a legend onto this plot with scale_color_gradient(), which is particularly helpful if the goal is to highlight a trend over the spread of the time series. # Scatterplot of state-level average annual temperature by avg annual precipitation (values and anomalies) # Data source: https://www.ncdc.noaa.gov/cag/statewide/time-series # Need to set base period from 1978 to 1998 in Options dialog and set parameters before downloading (12-Month) ca.pcp &lt;- read.csv(&#39;data/4-pcp-12-12-1978-2020.csv&#39;, skip=4) ca.tavg &lt;- read.csv(&#39;data/4-tavg-12-12-1978-2020.csv&#39;, skip=4) # Reassign the column names names(ca.pcp) &lt;- c(&#39;Year&#39;, &#39;Precip&#39;, &#39;Precip.Anomaly&#39;) names(ca.tavg) &lt;- c(&#39;Year&#39;, &#39;Temp&#39;, &#39;Temp.Anomaly&#39;) # Perform a left join and filter the data ca.weather &lt;- left_join(ca.tavg, ca.pcp, by=&#39;Year&#39;) %&gt;% mutate(Year=as.integer(substr(Year, 1, 4)), Period=if_else(Year&lt;1990, &#39;Before 1990&#39;, if_else(Year&lt;2000, &#39;1990-2000&#39;, if_else(Year&lt;2010, &#39;2000-2010&#39;, &#39;Since 2010&#39;)))) %&gt;% mutate(Period=factor(Period, c(&#39;Before 1990&#39;, &#39;1990-2000&#39;, &#39;2000-2010&#39;, &#39;Since 2010&#39;))) # Produce the base ggplot s &lt;- ggplot(data = ca.weather, aes(x=Temp.Anomaly, y=Precip.Anomaly)) hottest.v &lt;- max(ca.weather$Temp.Anomaly) hottest.y &lt;- filter(ca.weather, Temp.Anomaly==hottest.v)$Year driest.v &lt;- min(ca.weather$Precip.Anomaly) driest.y &lt;- filter(ca.weather, Precip.Anomaly==driest.v)$Year # Print the plot with all of the adjustments s + geom_point(aes(color=Period), size=2) + geom_smooth() + geom_label(label=if_else(ca.weather$Temp.Anomaly==hottest.v, paste(&#39;Hottest Year\\n&#39;, hottest.y), NULL), nudge_x = -.2, nudge_y = 3) + geom_label(label=if_else(ca.weather$Precip.Anomaly==driest.v, paste(&#39;Driest Year\\n&#39;, driest.y), NULL), nudge_x = -.4, nudge_y = 0) + labs(title=&#39;Hotter and Drier Wildfire Seasons in California&#39;, subtitle=&#39;Precipitation vs. temperature anomalies in California from January to October over 40 years (1978-2020)&#39;, caption=&#39;Data from NOAA&#39;, y=&#39;Precipitation anomaly (inches)&#39;, x=&#39;Temperature anomaly (°F)&#39;) + theme_light() # Save as png file # ggsave(file = &#39;CA.weather.png&#39;, height=9, width=12) # Produce our second, simpler scatterplot ggplot(ca.weather, aes(Year, Temp.Anomaly)) + geom_line(color=&#39;#0D47A1&#39;) + geom_point(aes(color=Temp.Anomaly)) + geom_hline(yintercept=0, linetype=2, color=&#39;#aaaaaa&#39;) + labs(title=&#39;California Temperature Anomaly 1978-2020&#39;) + scale_color_gradient(low=&#39;blue&#39;, high=&#39;red&#39;) + theme_classic() 6.2 Animated graph with gganimate To make a plot dynamically animated for insertion into a webpage, presentation, or any platform that supports gif images, we can simply add gganimate to the plot’s code. After choosing the aesthetic elements of the plot, write the transition_reveal() code, and clarify the sequence along which points should emerge, to get the result that we see below. library(gganimate) animated.plot &lt;- ggplot(ca.weather, aes(Year, Temp.Anomaly)) + geom_line(color=&#39;#0D47A1&#39;) +geom_point(aes(color=Temp.Anomaly)) + geom_hline(yintercept=0, linetype=2, color=&#39;#aaaaaa&#39;) + labs(title=&#39;California Temperature Anomaly 1978-2020&#39;) + scale_color_gradient(low=&#39;blue&#39;, high=&#39;red&#39;) + theme_classic() + # labels geom_label(aes(label=as.character(Year)), nudge_y=.3) + # gganimate code transition_reveal(seq_along(Year)) # animate(animated.plot) gganim 6.3 Interactive graph with plotly Creating an interactive version of the previous graphs is pretty simple using plotly. You just have to separate the data by group, which we do in the plot below through a series of filter commands from tidyverse. The benefit to creating an interactive graph is that the user can explore specific data, compare data by hovering over boxes, or move about the plot itself. This level of interactivity makes their reception of the data more nuanced, and may answer questions that a static plot would not. 6.3.1 plotly Example 1: Interactive version of the stacked bar library(plotly) # Split and filter the data Q31.yes &lt;- Q31_data %&gt;% filter(Q31==&#39;Yes&#39;) Q31.notsure &lt;- Q31_data %&gt;% filter(Q31==&#39;Not Sure&#39;) Q31.no &lt;- Q31_data %&gt;% filter(Q31==&#39;No&#39;) # Produce the plotly object plot_ly(x=Q31.yes$S5_cluster, y=Q31.yes$freq.wgt, type=&#39;bar&#39;, name=&#39;Yes&#39;, marker = list(color = &#39;#DC7633&#39;), text = Q31.yes$freq.wgt, textposition = &#39;auto&#39;) %&gt;% add_trace(x=Q31.notsure$S5_cluster, y=Q31.notsure$freq.wgt, name=&#39;Not Sure&#39;, marker = list(color = &#39;#5D6D7E&#39;)) %&gt;% add_trace(x=Q31.no$S5_cluster, y=Q31.no$freq.wgt, name=&#39;No&#39;, marker = list(color = &#39;#5499C7&#39;)) %&gt;% layout(barmode = &#39;stack&#39;) You can find more examples of using plotly on their website. 6.4 Web app with RShiny This Shiny tutorial was edited based on the official tutorial on the website With RShiny, it is possible to make functions in your R script available to people who do not necessarily know R. For example, in this app, you can create different types of graphs by selecting site, plot type, and other parameters. It uses the power of R libraries to create interactive plots for exploration. Just like any other web tool, an RshinyApp has components on the User and Server side. The visual appearance of the app can be modified in the user component. You can change layout, font size, color, etc. On the server side, you can customize how your app responds to user inputs or interactions, what options to make available, and what the default presentation is. If you are running this code simultaneously in your own RStudio window, the code below will generate a new panel in which the ShinyApp object lives. Because we have not built it out yet, you will see an empty window if you run the code below: A basic ShinyApp starter template looks like this: library(shiny) ui &lt;- fluidPage() server &lt;- function(input, output){} shinyApp(ui=ui, server=server) Some content will show up when you add some (text) elements with fluidPage, as we do below. ui &lt;- fluidPage(&#39;Hello world&#39;) server &lt;- function(input, output){} shinyApp(ui=ui, server=server) 6.4.1 Input functions For richer content, the shiny package has built-in functions that allow you to insert various applications, information, and interactive elements. We will demonstrate one of them, sliderInput, which adds a slider into your web app. # Generate a user slider from 1 to 100 ui &lt;- fluidPage( sliderInput(inputId=&#39;num&#39;, label=&#39;Choose a number&#39;, value=25, min=1, max=100) ) server &lt;- function(input, output){} shinyApp(ui=ui, server=server) sliderInput is part of a group of functions called inputs. These functions allow an Rshiny developer to add html elements which gather and record user input. Here are some input functions you can try: All input functions require 2 arguments: inputId and label. inputId is an ID that R will use when it creates the input element in a webpage, so that it can be referenced later. label is text that the user will see, describing what the input element is meant to be used for. The rest of the arguments are function-specific and can be explored at the full tutorial we linked earlier. 6.4.2 Output functions Output functions allow you to add outputs of R code into your web page. These outputs can be images, plots, tables, text, or any other result of an R command. There are specific output functions for each type of output. Rshiny output functions Output functions have a similar set of syntactic rules to the input functions: Here is the code for our app, now including an output function. Running this script won’t display any plot. It just reserves a space in the webpage for a plot with the assigned id ‘hist’. To actually create the plot, you must use the server function. # Produce a shinyApp with an output function ui &lt;- fluidPage( sliderInput(inputId=&#39;num&#39;, label=&#39;Choose a number&#39;, value=25, min=1, max=100), plotOutput(&#39;hist&#39;) ) server &lt;- function(input, output){} shinyApp(ui=ui, server=server) 6.4.3 Server function Server functions facilitate the interactivity in your web application. This is where you set up how an output (ex: graph) changes with the user input (ex: some number). However, there are 3 rules that have to be followed, which we have printed in this script: function(input, output){ #1 outputId is the outputId you defined with output function #2 To render a plot on a webpage, use renderPlot() output$outputId &lt;- renderSomething({ #3 inputId is the inputId you defined with input function someFunction(input$inputId) }) } # Anything inside the curly braces is an R code. And it can be multiple lines of code. You must define your output variable using outputId prefixed with output$ then call the R script that will produce the output with a render*({}) function. After this is done, you must call the user input with its inputId and prefix it with input$. In our example script, we render a plot as follows: # Save the ui as &#39;ui&#39; ui &lt;- fluidPage( # Generate the slider input, give it a label and range sliderInput(inputId=&#39;num&#39;, label=&#39;Choose a number&#39;, value=25, min=1, max=100), # Plot the output as a histogram plotOutput(&#39;hist&#39;) ) # Define the function of our server server &lt;- function(input, output){ # Create the call for renderPlot() output$hist &lt;- renderPlot({ # Give a title and histogram command, with input from the slider title &lt;- paste(input$num, &#39;random normal values&#39;) hist(rnorm(input$num), main=title, xlab=&#39;values&#39;) }) } # Produce the Rshiny app shinyApp(ui=ui, server=server) To make more advanced applications, we highly encourage you to read the instructions on the Rshiny website. 6.4.4 Sharing your app You now possess the tools you need to create your own Shiny app. In its current form, it will only run on your computer, which means nobody else has access to it. To make it available to the public, you must: Save the ui and server objects/scripts into their own R script, name this script app.R, and save it in a separate folder. The name must be app.R as that is the file that the server will look for when you deploy your app. For this example, we have saved the app we’ve been working on as app.R # Saved version of our shinyApp ui &lt;- fluidPage( sliderInput(inputId=&#39;num&#39;, label=&#39;Choose a number&#39;, value=25, min=1, max=100), plotOutput(&#39;hist&#39;) ) server &lt;- function(input, output){ output$hist &lt;- renderPlot({ title &lt;- paste(input$num, &#39;random normal values&#39;) hist(rnorm(input$num), main=title, xlab=&#39;values&#39;) }) } Go to shinyapps.io and log in or sign up for an account. Run the app on your computer In the top right cover of R viewer window, there will now be a Publish button that you can use to publish your app. Follow the prompted instructions to publish. 6.5 More data visualization resources More data visualization resources can be found in this spreadsheet. "],
["spatial.html", "Section 7 Spatial data 7.1 Importing spatial data 7.2 Processing 7.3 Some Visualization Examples", " Section 7 Spatial data Spatial data refers to any dataset that associates specific recorded values with a unique region of space. This can be latitude/longitude, state or city name, country, or any other spatial information. In R, spatial data comes in a variety of formats. Amongst the most common are shapefiles, geodatabases, and geojson files. All of these data can be read and processed into a dataframe just like common data formats (csv, excel, etc.). However, we often have to use special libraries to do so. In addition, because of the nature of this data, spatial relationship operations can also be performed to join or create new features. Later in this tutorial, we will generate some examples using the sf and raster libraries to perform these analyses. 7.1 Importing spatial data In this first section, we will use two libraries, ‘dplyr’ and ‘viridis’. In general, the most common forms of geospatial data are shapefiles for vectors and tiffs for rasters. This tutorial will focus primarily on these 2 types of data and how to manipulate them. 7.1.1 Reading shapefile The shapefile format consists of at least 3 files: the main file (.shp), the index file (.shx), and the table file (.dbf). Sometimes there is also a projection file (.prj). Please visit this webpage for more details on the distinctions and importance of each of these elements. Because of this multi-file structure, it is good practice to save your shapefile in a folder or .zip file with its associated index file and table file. When reading the data into R, we can simply point to the .shp file in this folder and the code will know how to use the other files. In a similar way to how we read in csv files in the first page of instruction, we will use the sf() package to read the shapefile. ## Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0 ## tibble [27,459 × 11] (S3: sf/tbl_df/tbl/data.frame) ## $ ID : int [1:27459] 1 2 3 4 5 6 7 8 9 10 ... ## $ TSEVENT_ID: int [1:27459] 3 9 10 10 10 10 10 11 27 27 ... ## $ YEAR : int [1:27459] -1610 -479 -426 -426 -426 -426 -426 -373 142 142 ... ## $ MONTH : int [1:27459] NA NA 6 6 6 6 6 NA NA NA ... ## $ DAY : int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ DATE_STRIN: chr [1:27459] &quot;-1610/??/??&quot; &quot;-0479/??/??&quot; &quot;-0426/06/??&quot; &quot;-0426/06/??&quot; ... ## $ ARR_DAY : int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ ARR_HOUR : int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ ARR_MIN : int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ TRAV_HOURS: int [1:27459] NA NA NA NA NA NA NA NA NA NA ... ## $ geometry :sfc_POINT of length 27459; first list element: &#39;XY&#39; num [1:2] 2782987 4232038 ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA NA NA NA NA NA NA ## ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;ID&quot; &quot;TSEVENT_ID&quot; &quot;YEAR&quot; &quot;MONTH&quot; ... When looking at the dataframe structure above, we see that the shapefile is processed similarly to how a regular dataframe is processed, except for the additional geometry field which contains the spatial information. If we want to view the data, it will show up much as a regular csv would, with rows representing observations and columns representing features. If we want to plot this data, we can simply use the base plot function. # a plot of the first 4 columns # plotted as location plot(tsunami[, 1:4]) # a plot of only columns # plotted as values without geometry plot(tsunami$DEATHS) plot(tsunami$LONGITUDE, tsunami$DIST_FROM_) If you only want to plot the geometry (points): plot(st_geometry(tsunami)) 7.1.2 Reading raster Raster .tif files can be imported with the raster library. library(raster) # Import the raster file wind_jan &lt;- raster(&#39;data/wc2.1_10m_wind/wc2.1_10m_wind_01.tif&#39;) # Plot the file plot(wind_jan) 7.2 Processing 7.2.1 Creating point data This example shows how to convert data with latitude and longitude coordinates into a spatial dataframe. We’ll use the charcoal records from the GCD package as our field. library(GCD) data(&quot;paleofiresites&quot;) df.table &lt;- paleofiresites wgs84crs &lt;- 4269 sf.table &lt;- st_as_sf(df.table, coords=c(&#39;long&#39;, &#39;lat&#39;), crs=wgs84crs, remove=F) The code above helps us to access the data of charcoal records, save them into a dataframe, and use the st_as_sf() function to transform the data into a shapefile by encoding the ‘long’ and ‘lat’ columns as the coordinate pairs for each observation. We gave the argument remove=F so that the function would not then take these points out of the data, so that we can continue to see them. By saving what was a regular dataframe as a shapefile, we can now see the difference in how plot() treats the data. Below we give two identical calls, one on the df.table object and the other on the transformed sf.table() object. What you will see is that the first call produces the usual matrix plot we expect when we ask plot() to look at multiple columns in a dataset. The second call gives us points plotted according to their geospatial coordinates. With a dataset large enough and broad enough, these data begin to resemble a global map: # non spatial data plot(df.table[,c(7,11:13)]) # spatial data plot(sf.table[,c(7,11:13)]) 7.2.2 Joining with another dataframe Oftentimes, we want to join non-spatial data with their geographic location. For example, if we have information on energy production and consumption by state, we can join it with a spatial feature for better visualization. To do this, we use a function called ‘inner_join’, which is one of four possible types of joining. With an inner join, we only include the points from both datafiles that overlap. If you are curious about the distinctions between different types of joins, more information can be found towards the end of the ‘Structuring’ page earlier in our collection. # Get energy data energy.data &lt;- read.csv(&#39;data/energy-by_state.csv&#39;) # Use USABoundaries library to get US spatial data library(USAboundaries) us.geo &lt;- us_states(resolution=&#39;low&#39;) joined.data &lt;- us.geo %&gt;% inner_join(energy.data, by=c(&#39;state_abbr&#39;=&#39;State&#39;)) joined.data.albers &lt;- st_transform(joined.data, crs=5070) # Plot the data plot(joined.data.albers[,&#39;Production..U.S..Share&#39;]) 7.2.3 Spatial joining If we look at the paleofiresites data, we can see that we do not have a US state corresponding to each point location. We can create this variable with a spatial join. # Get all paleo sites for the US us.paleo &lt;- paleofiresites %&gt;% filter(country==&#39;USA&#39;) %&gt;% st_as_sf(coords=c(&#39;long&#39;, &#39;lat&#39;), crs=wgs84crs) # Set data to the same coordinate system us.paleo.albers &lt;- st_transform(us.paleo, crs=5070) us.geo.albers &lt;- st_transform(us.geo, crs=5070) # Perform a spatial join within sp.join &lt;- st_join(us.paleo.albers, us.geo.albers, join=st_within) # Produce the plot {plot(st_geometry(us.geo.albers)) plot(sp.join[, &#39;state_name&#39;], add=T, pch=16)} For more information on what you can do with a vector file in R, please visit the sf library reference or read the book Geocomputation with R. 7.3 Some Visualization Examples Now that we have walked through how to read in, manipulate, and clean spatial data, we are going to run through a few key examples of visualization. Below we use the rnaturalearth() package, in coordination with the ggplot2() and ggrepel() libraries, to generate a few examplesm=, so that you can see the breadth of what plotting spatial data with R can accomplish. 7.3.1 Plotting charcoal data # Get a world map from rnaturalearth library library(rnaturalearth) world &lt;- ne_countries(scale=&#39;small&#39;, returnclass=&#39;sf&#39;) # Project to robinson projection sf.world.robin &lt;- st_transform(world, crs=&#39;+proj=robin&#39;) # Use imported graticule data for viz grat &lt;- st_geometry(read_sf(&#39;data/ne_110m_graticules_all/ne_110m_wgs84_bounding_box.shp&#39;)) # Get data for paleo sites paleosites &lt;- paleofiresites %&gt;% mutate(site_name = trimws(as.character(site_name))) %&gt;% st_as_sf(coords=c(&#39;long&#39;, &#39;lat&#39;), crs=4326) %&gt;% st_transform(crs=&#39;+proj=robin&#39;) newCoods &lt;- st_coordinates(paleosites) paleosites$Lon &lt;- newCoods[,1] paleosites$Lat &lt;- newCoods[,2] # Set ggplot theme library(ggplot2) library(ggrepel) ggTheme &lt;- theme( legend.position=&#39;none&#39;, panel.grid.minor = element_blank(), panel.grid.major = element_blank(), panel.background = element_blank(), plot.background = element_rect(fill=&#39;#e6e8ed&#39;), panel.border = element_blank(), axis.line = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), plot.title = element_text(size=22) ) # Pick a few sites to be highlighted selected.sites &lt;- c( &#39;Little Molas Lake&#39;,&#39;Xindi&#39;,&#39;Laguna Oprasa&#39;,&#39;Morro de Itapeva&#39;, &#39;Aracatuba&#39;, &#39;Lago del Greppo&#39;,&#39;Nursery Swamp&#39;, &#39;Huguangyan Maar Lake&#39; ) # Complete the mapping map &lt;- ggplot(data=sf.world.robin) + geom_sf(data=grat, fill=&#39;white&#39;, color=&#39;white&#39;) + geom_sf(fill=&#39;#c1cdcd&#39;, size=.1, color=&#39;white&#39;) + geom_point( data=paleosites, aes(x=Lon, y=Lat), color=&#39;#009ACD&#39;,size=1) + geom_point( data=paleosites %&gt;% filter(site_name %in% selected.sites), aes(x=Lon, y=Lat), color=&#39;#FF0000&#39;,size=2) + geom_text_repel( data=paleosites %&gt;% filter(site_name %in% selected.sites), aes(x=Lon, y=Lat, label=site_name), color=&#39;#000000&#39;, size=4) + coord_sf(datum=st_crs(54030)) + ggTheme # Print the map map "],
["analysis.html", "Section 8 Analysis 8.1 Working with survey data 8.2 Working with COVID-19 data", " Section 8 Analysis 8.1 Working with survey data We’ll use a survey data from a study of approach to engage christians in the issue of global warming. Using this data allow us to compare our results with the original. In this exercise, we’ll replicate the application of sampling weights and the standardization of variables which are common treatments of raw survey data. ## Selected variables ## # A tibble: 13 x 2 ## V1 V2 ## &lt;chr&gt; &lt;chr&gt; ## 1 subjID &quot;Participant ID … ## 2 GWHap &quot;On a scale from 1 to 7, how strongly do you believe that global… ## 3 GWworry &quot;How worried are you about global warming? … ## 4 ex_ppgender &quot;What is your gender? … ## 5 ex_ppage &quot;What is your age? … ## 6 raceCat &quot;Race … ## 7 ex_ppstaten &quot;What U.S. State do you live in? Or do you live in a U.S. Territ… ## 8 moral_pre &quot;TOPIC 10: THE ENVIRONMENT\\nIn your opinion, how much do you thi… ## 9 relig_pre &quot;TOPIC 10: THE ENVIRONMENT\\nIn your opinion, how much do you thi… ## 10 health_pre &quot;TOPIC 10: THE ENVIRONMENT\\nIn your opinion, how much do you thi… ## 11 moral_post &quot;In your opinion, how much do you think environmental protection… ## 12 relig_post &quot;In your opinion, how much do you think environmental protection… ## 13 health_post &quot;In your opinion, how much do you think environmental protection… 8.1.1 Variable Standardization Standardizing variables must be performed before any regression or other statistical analysis are done with the data. It can be done by rescaling the variables using the z-score formula. Standardized variables will then have a mean of 0 and a standard deviation of 1. # z-score formula get.z.score &lt;- function(X){ (X-mean(X, na.rm=T))/sd(X, na.rm=T) } # standardizing the pretest and posttest variables Zdata &lt;- data %&gt;% mutate(across(ends_with(&#39;_pre&#39;)|ends_with(&#39;_post&#39;), get.z.score, .names=&#39;Z{col}&#39;)) Zdata ## # A tibble: 1,620 x 19 ## subjID GWHap GWworry ex_ppgender ex_ppage raceCat ex_ppstaten moral_pre ## &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; ## 1 101 7 [I s… 6 [6] 2 [Female] 69 2 [Bla… 36 [Ohio] 4 [4] ## 2 102 4 [I a… 3 [3] 1 [Male] 30 1 [Whi… 41 [South … 4 [4] ## 3 103 6 [6] 7 [I a… 2 [Female] 31 1 [Whi… 36 [Ohio] 5 [5] ## 4 104 7 [I s… 6 [6] 2 [Female] 41 1 [Whi… 43 [Tennes… 7 [Very … ## 5 105 2 [2] 2 [2] 2 [Female] 28 1 [Whi… 14 [Illino… 4 [4] ## 6 106 5 [5] 4 [4] 2 [Female] 52 1 [Whi… 33 [New Yo… 4 [4] ## 7 107 5 [5] 6 [6] 1 [Male] 36 4 [His… 36 [Ohio] 7 [Very … ## 8 108 7 [I s… 6 [6] 2 [Female] 19 3 [Oth… 33 [New Yo… 6 [6] ## 9 109 6 [6] 5 [5] 1 [Male] 21 2 [Bla… 34 [North … 5 [5] ## 10 110 7 [I s… 5 [5] 1 [Male] 26 1 [Whi… 25 [Missis… 6 [6] ## # … with 1,610 more rows, and 11 more variables: relig_pre &lt;dbl+lbl&gt;, ## # health_pre &lt;dbl+lbl&gt;, moral_post &lt;dbl+lbl&gt;, relig_post &lt;dbl+lbl&gt;, ## # health_post &lt;dbl+lbl&gt;, Zmoral_pre &lt;dbl&gt;, Zrelig_pre &lt;dbl&gt;, ## # Zhealth_pre &lt;dbl&gt;, Zmoral_post &lt;dbl&gt;, Zrelig_post &lt;dbl&gt;, Zhealth_post &lt;dbl&gt; 8.1.2 Sample weighting Per Josep Espaga Reig’s book, there are 4 steps in survey weighting: Base/design weights: if participants have different probability of being sampled; Non-response weights: if some participants didn’t respond to the survey; Use of auxillary data/calibration: adjusting weights to the total population; Analysis of weight variability/trimming: to check variability in the computed weights; In this example, we will estimate of the percent of US christian population who either believe global warming (GW) is happening or worried about GW. We will give descriptives of the distribution of these two variables and then a simple extrapolation and compute total for the whole US christian population. Our variables of interest are: GWHap: On a scale from 1 to 7, how strongly do you believe that global warming is or is not happening? GWworry: How worried are you about global warming? 8.1.2.1 Step 1: Design weights In the paper, it is mentionned that participants were recruited via Prime Panels. And only those who previously identified as christians were selected. It should be good to assume that each participants had an equal probability of being sampled. Therefore, we can skip the Step 1 of the weighting. If we had to compute the design weight \\(d_i\\), it would be equal to the inverse of the probability of being sampled: \\(1/p_i\\). Per Josep Espaga Reig’s book: the design weights can be interepreted as “the number of units in our population that each unit in our sample represents. […] The sum of all design weights should be equal to the total number of units in our population”. 8.1.2.2 Step 2: Non-response weights The paper also doesn’t mention any information about non-respondent during the survey. So we can assume that there is no bias in the responses and we can skip Step 2. In short, the goal of Step 2 is to account for differences in propensity to respond among the participants. For example, participants from specific neighborhood, income level, etc may have lower response rates, and all the response we have are from a specific demographic groups, which would bias the analysis. 8.1.2.3 Step 3: Using auxillary data for weight calibration The goal here is to calibrate the survey data to the population in general. So we would need information on both the survey respondents and the population. In this case, we will use demographic variables (gender, education, and race). One of the most common method for calibration weighting is called: raking which we will use as an the example. We’ll use as auxiliary data the PEW Datasets which gives some statistics of the US christian population. We’ll scrape the data directly from their website so that we’ll have fewer data wrangling to do. # PEW data # getting only the christian respondent and their weights pew.data &lt;- read_sav(&#39;data/pew-data.sav&#39;) %&gt;% select(RELTRAD, agerec, SEX, racethn, WEIGHT) %&gt;% mutate(across(where(is.labelled), as_factor)) %&gt;% mutate(across(where(is.factor), as.character)) %&gt;% mutate( religion = RELTRAD %&gt;% recode( `Evangelical Protestant Tradition`=&#39;christian&#39;, `Mainline Protestant Tradition`=&#39;christian&#39;, `Historically Black Protestant Tradition`=&#39;christian&#39;, `Catholic`=&#39;christian&#39;, `Mormon`=&#39;christian&#39;, `Orthodox Christian`=&#39;christian&#39;, `Jehovah&#39;s Witness`=&#39;christian&#39;, `Other Christian`=&#39;christian&#39;, `Jewish`=&#39;other&#39;, `Muslim`=&#39;other&#39;, `Buddhist`=&#39;other&#39;, `Hindu`=&#39;other&#39;, `Other World Religions`=&#39;other&#39;, `Other Faiths`=&#39;other&#39;, `Unaffiliated (religious &quot;nones&quot;)`=&#39;other&#39;, `Don&#39;t know/refused - no information on religious identity`=&#39;other&#39;, ), pew.age = agerec %&gt;% recode( `Age 24 or younger`=&#39;18-29&#39;, `Age 25-29`=&#39;18-29&#39;, `30-34`=&#39;30-49&#39;, `35-39`=&#39;30-49&#39;, `40-44`=&#39;30-49&#39;, `45-49`=&#39;30-49&#39;, `50-54`=&#39;50-64&#39;, `55-59`=&#39;50-64&#39;, `60-64`=&#39;50-64&#39;, `65-69`=&#39;65+&#39;, `70-74`=&#39;65+&#39;, `75-79`=&#39;65+&#39;, `80-84`=&#39;65+&#39;, `85-89`=&#39;65+&#39;, `Age 90 or older`=&#39;65+&#39;, `Don&#39;t know/refused`=&#39;NA&#39; ), pew.race = racethn %&gt;% recode( `White non-Hispanic`=&#39;White&#39;, `Black non-Hispanic`=&#39;Black&#39;, `Hispanic`=&#39;Hispanic&#39;, `Other`=&#39;Other&#39;, `Don’t know/Refused (VOL.)`=&#39;NA&#39; ) ) %&gt;% filter(religion==&#39;christian&#39;) %&gt;% select(SEX, pew.age, pew.race, WEIGHT) %&gt;% na.omit() names(pew.data)[1] &lt;- &#39;pew.gender&#39; head(pew.data,10) ## # A tibble: 10 x 4 ## pew.gender pew.age pew.race WEIGHT ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Female 65+ White 0.438 ## 2 Female 65+ White 0.384 ## 3 Female 50-64 White 0.663 ## 4 Female 30-49 Hispanic 1.64 ## 5 Female 50-64 White 0.357 ## 6 Female 65+ Black 0.664 ## 7 Female 65+ White 1.42 ## 8 Male 30-49 White 0.388 ## 9 Male 65+ Other 0.238 ## 10 Female 30-49 White 0.882 pew.gender_age &lt;- pew.data %&gt;% select(pew.age, pew.gender, WEIGHT) %&gt;% group_by(pew.gender, pew.age) %&gt;% summarise(n=sum(WEIGHT)) %&gt;% tidyr::pivot_wider(names_from=pew.age, values_from=n) %&gt;% select(-`NA`) pew.race &lt;- pew.data %&gt;% select(pew.race, WEIGHT) %&gt;% group_by(pew.race) %&gt;% summarise(n=sum(WEIGHT)) %&gt;% filter(pew.race!=&#39;NA&#39;) Now we’ll recode our age and race variables to match the PEW data. # interaction between gender and age our_data.recoded &lt;- data %&gt;% select(ex_ppage, ex_ppgender, raceCat) %&gt;% mutate(across(where(is.labelled), as_factor)) %&gt;% mutate( age_group=cut(ex_ppage %&gt;% as.numeric(), breaks=c(18, 30, 50, 65, 100), right=F) ) %&gt;% mutate(across(where(is.factor), as.character)) %&gt;% mutate( age_group=age_group %&gt;% recode( `[18,30)`=&#39;18-29&#39;, `[30,50)`=&#39;30-49&#39;, `[50,65)`=&#39;50-64&#39;, `[65,100)`=&#39;65+&#39; ), race_cat=raceCat %&gt;% recode( `White Non-hispanic`=&#39;White&#39;, `Black Non-hispanic`=&#39;Black&#39;, `Hispanic`=&#39;Hispanic&#39;, `Other Non-hispanic`=&#39;Other&#39;, `Two+ races non-hispanic`=&#39;Other&#39; ) ) %&gt;% tidyr::unite(col=gender_age, ex_ppgender, age_group, remove=F) %&gt;% mutate(gender_age=replace(x=gender_age, list=gender_age %in% c(&#39;Female_NA&#39;, &#39;Male_NA&#39;), values=NA)) %&gt;% na.omit() # number of participants/weights by units # this would be the sum of the final weights from Step 1 and 2 if they were necessary our_data.gender_age &lt;- our_data.recoded %&gt;% group_by(age_group, ex_ppgender) %&gt;% summarise(n = n()) %&gt;% tidyr::pivot_wider(names_from=age_group, values_from=n) our_data.race &lt;- our_data.recoded %&gt;% group_by(race_cat) %&gt;% summarise(n = n()) rm(list=setdiff(ls(), c(&#39;data&#39;, &#39;data.variables&#39;, &#39;Zdata&#39;, &#39;pew.data&#39;, &#39;pew.gender_age&#39;, &#39;pew.race&#39;, &#39;our_data.recoded&#39;, &#39;our_data.gender_age&#39;, &#39;our_data.race&#39;))) Now, we’ll scale the PEW data to our sample size. # our total (weighted) observations our_data.wgt &lt;- nrow(our_data.recoded) # scaling PEW to our data # for age and gender pew.gender_age.scaled &lt;- pew.gender_age %&gt;% tidyr::pivot_longer(-pew.gender, names_to=&#39;pew.age&#39;, values_to=&#39;WEIGHT&#39;) %&gt;% tidyr::unite(col=gender_age, pew.gender, pew.age) total.pew &lt;- pew.gender_age.scaled$WEIGHT %&gt;% sum() pew.gender_age.scaled %&lt;&gt;% mutate(Freq = round(WEIGHT/total.pew * our_data.wgt, 0) ) %&gt;% select(-WEIGHT) pew.gender_age.scaled ## # A tibble: 8 x 2 ## gender_age Freq ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female_18-29 149 ## 2 Female_30-49 289 ## 3 Female_50-64 250 ## 4 Female_65+ 197 ## 5 Male_18-29 131 ## 6 Male_30-49 245 ## 7 Male_50-64 218 ## 8 Male_65+ 141 # for race pew.race.scaled &lt;- pew.race %&gt;% mutate( Freq = round(n/total.pew * our_data.wgt, 0), race_cat=pew.race ) %&gt;% select(-n, -pew.race) pew.race.scaled ## # A tibble: 4 x 2 ## Freq race_cat ## &lt;dbl&gt; &lt;chr&gt; ## 1 209 Black ## 2 262 Hispanic ## 3 85 Other ## 4 1071 White Implementing calibration We’ll use the same procedure explained in this book which uses the survey package. # No weights or probabilities supplied, assuming equal probability our.svydesign &lt;- svydesign(ids = ~ 0, data=our_data.recoded) # the variable used for calibration are: gender/age interaction and race our.raked &lt;- rake(our.svydesign, sample.margins = list(~race_cat, ~gender_age), population = list(pew.race.scaled, pew.gender_age.scaled)) # collecting the weights raked.weight &lt;- our.raked$postStrata[[1]][[1]] %&gt;% attributes() %&gt;% .[[&quot;weights&quot;]] weighted_data &lt;- our_data.recoded weighted_data$raked.weight &lt;- raked.weight head(weighted_data, 10) ## # A tibble: 10 x 7 ## ex_ppage gender_age ex_ppgender raceCat age_group race_cat raked.weight ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 69 Female_65+ Female Black Non-h… 65+ Black 2.11 ## 2 30 Male_30-49 Male White Non-h… 30-49 White 0.959 ## 3 31 Female_30-… Female White Non-h… 30-49 White 0.688 ## 4 41 Female_30-… Female White Non-h… 30-49 White 0.688 ## 5 28 Female_18-… Female White Non-h… 18-29 White 0.361 ## 6 52 Female_50-… Female White Non-h… 50-64 White 0.875 ## 7 36 Male_30-49 Male Hispanic 30-49 Hispanic 2.41 ## 8 19 Female_18-… Female Other Non-h… 18-29 Other 0.531 ## 9 21 Male_18-29 Male Black Non-h… 18-29 Black 1.29 ## 10 26 Male_18-29 Male White Non-h… 18-29 White 0.945 8.2 Working with COVID-19 data In this example, we’ll use daily updated data from the JHU Covid Resource Center to create animated time series of coronavirus cases and deaths in the US. The data is available in their GitHub. We’ll use the daily report for the US from 04-12-2020 to 06-22-2020. repo.url &lt;- &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/&#39; # importing all csv us.covid.files &lt;- list() date.format &lt;- &#39;%m-%d-%Y&#39; dates &lt;- format(seq(as.Date(&#39;2020/04/12&#39;), as.Date(as.Date(&#39;2020/06/22&#39;)), by=&#39;day&#39;), date.format) i &lt;- 1 for (date in dates){ us.covid.files[[i]] &lt;- read.csv(url(paste0(repo.url, date, &#39;.csv&#39;))) %&gt;% select(Province_State, Confirmed, Deaths, Recovered, Mortality_Rate) %&gt;% mutate(Date=as.Date(date, date.format), Confirmed.rank=rank(-Confirmed), Deaths.rank=rank(-Deaths), Mortality.rank=rank(-Mortality_Rate)) i &lt;- i+1 } head(us.covid.files[[1]]) ## Province_State Confirmed Deaths Recovered Mortality_Rate Date ## 1 Alabama 3563 93 NA 2.610160 2020-04-12 ## 2 Alaska 272 8 66 2.941176 2020-04-12 ## 3 Arizona 3542 115 NA 3.246753 2020-04-12 ## 4 Arkansas 1280 27 367 2.109375 2020-04-12 ## 5 California 22795 640 NA 2.812020 2020-04-12 ## 6 Colorado 7307 289 NA 3.955112 2020-04-12 ## Confirmed.rank Deaths.rank Mortality.rank ## 1 22 27 33 ## 2 51 47 27 ## 3 23 21 24 ## 4 39 39 43 ## 5 6 7 29 ## 6 16 14 14 us.covid.df &lt;- do.call(rbind, us.covid.files) # line series by state last.df &lt;- us.covid.files[[length(us.covid.files)]] us.covid.plot &lt;- ggplot(us.covid.df %&gt;% filter(Confirmed.rank &lt;=10), aes(y=Confirmed.rank, group=Province_State, fill=Province_State)) + geom_tile(aes(x=Confirmed/2, width=Confirmed, height=0.9), alpha=0.8, color=NA) + geom_text(aes(x=0, label=paste(Province_State, &#39; &#39;)), hjust=1, vjust=1) + geom_text(aes(x=Confirmed, label=paste0(&#39; &#39;, round(Confirmed/1000), &#39;k&#39;)), hjust=0) + scale_y_reverse() + coord_cartesian(clip=&#39;off&#39;) + theme_bw() + # custom ggtheme theme(legend.position=&#39;none&#39;, axis.line=element_blank(), axis.title=element_blank(), axis.ticks=element_blank(), axis.text=element_blank(), panel.grid.major.y=element_blank(), panel.grid.minor.y=element_blank(), panel.border=element_blank(), plot.margin = margin(2,1,2,3,&#39;cm&#39;)) us.covid.anim &lt;- us.covid.plot + transition_states(Date, transition_length=4, state_length=1) + view_follow(fixed_x=T) + labs(title = &#39;Daily coronavirus cases : {closest_state}&#39;, subtitle = &#39;Top 10 States&#39;, caption =&#39;Data Source: Johns Hopkins Coronavirus Resource Center&#39;) # animate(us.covid.anim, nframes=400, fps=10, renderer=gifski_renderer(&#39;images/us-covid-rank.gif&#39;)) us-covid-rank The previous animated plot shows the the evolution of the number of cases. And changing to other variables such as Mortality_Rate or Deaths would follow the same steps. Now, if instead we want to show how the evolution of case change over space, we can do mapping instead. library(sf) # first we&#39;ll join with a geometry feature of the US us.states.geo &lt;- read_sf(&#39;data/us-states/us-states.shp&#39;) # adding geometry us.covid.sf &lt;- us.states.geo %&gt;% right_join(us.covid.df, by=c(&#39;state_name&#39;=&#39;Province_State&#39;)) # mapping max_val &lt;- max(us.covid.df$Mortality_Rate) mid_val &lt;- mean(us.covid.df$Mortality_Rate) map &lt;- ggplot(us.covid.sf) + geom_sf(aes(fill=Mortality_Rate), size=.1) + scale_fill_gradient2(high=&#39;#6E2C00&#39;, mid=&#39;#FDFEFE&#39;, low=&#39;#28B463&#39;, midpoint=mid_val, limits = c(0,max_val)) + coord_sf(crs=5070) + theme_bw() + # custom ggtheme theme(legend.position=&#39;bottom&#39;, axis.line=element_blank(), axis.title=element_blank(), axis.ticks=element_blank(), axis.text=element_blank(), panel.grid.major.y=element_blank(), panel.grid.minor.y=element_blank(), panel.border=element_blank(), plot.margin = margin(1,1,1,1,&#39;cm&#39;)) map.anim &lt;- map + transition_states(Date, transition_length=4, state_length=1) + labs(title = &#39;Daily coronavirus cases : {closest_state}&#39;, caption =&#39;Data Source: Johns Hopkins Coronavirus Resource Center&#39;) # animate(map.anim, nframes=100, fps=5, renderer=gifski_renderer(&#39;images/us-covid-map.gif&#39;)) us-covid-map "],
["resources.html", "Section 9 Resources 9.1 Data 9.2 EDS Course Map", " Section 9 Resources 9.1 Data Here is a non-exhaustive list of data resources for environmental studies. https://archive.ics.uci.edu/ml/datasets/Forest+Fires Region URL Topics World Cities http://www.worldclimate.com/ Temperature, Rainfall, Sea-level US https://climatecommunication.yale.edu/ Yale Program on Climate Change Communication US https://data.census.gov/cedsci/ Census surveys US https://www.epa.gov/ejscreen/download-ejscreen-data Environmental justice US https://www.data.gov/ Agriculture, Climate, Consumer, Ecosystems, Education, Energy, Finance, Health, Local Government, Manufacturing, Maritime, Ocean, Public Safety, Science &amp; Research US https://simplyanalytics.com/ Demography, Business, Health, Marketing US https://www.ncdc.noaa.gov/cdo-web/datatools Climate, Weather US https://www.epa.gov/outdoor-air-quality-data Air quality US https://www.americancommunities.org/methodology/ Communities US https://tidesandcurrents.noaa.gov/products.html Sea water levels US https://www.waterqualitydata.us/ Water quality US https://waterdata.usgs.gov/nwis/qw Water quality US https://hifld-geoplatform.opendata.arcgis.com/ Agriculture, Borders, Boundaries, Chemicals, Commercial, Communications, Education, Emergency Services, Energy, Finance, Food Industry, Geonames, Government, Law Enforcement, Mail Shipping, Mining, National Flood Hazard, Natural Hazards, Public Health, Public Venues, Transportation Air, Transportation Ground, Transportation Water, Water Supply OECD https://data.oecd.org/ Agriculture, Development, Economy, Education, Energy, Environment, Finance, Government, Health, Innovation and Technology, Jobs, Society Multpile https://www.kdnuggets.com/datasets/index.html Multiple Multiple https://www.kaggle.com/datasets Multiple Multiple https://data.fivethirtyeight.com/ Multiple Global https://csssi.yale.edu/ Yale CSSSI Global https://yceo.yale.edu/image-sources Yale Center for Earth Observation Global https://osf.io/ Multiple Global https://data.worldbank.org/ Development Global https://data.humdata.org/ Humanitarian Global https://climatedataguide.ucar.edu/climate-data Geospatial data resources Global http://chelsa-climate.org/downloads/ Climate predictions, timeseries Global https://ourworldindata.org/ Health, Demographic Change, Food and Agriculture, Education and Knowledge, Energy and Environment, Innovation and Technological Change, Poverty and Economic Development, Living conditions, Community and Wellbeing, Human rights and Democracy, Violence and War Global http://aqicn.org/data-platform/register/ Air quality Global https://globalfishingwatch.org/datasets-and-code/ Fishing Global https://www.gbif.org/ Biodiversity occurences Global https://data.globalforestwatch.org/ Forest Change, Land Use, Conservation, People, Forest Cover Global https://resourcewatch.org/data/explore Food, Climate, Energy, Water, Forests, Ocean, Society, Cities Europe https://www.eea.europa.eu/data-and-maps Air pollution, Biodiversity – Ecosystems, Chemicals, Climate change adaptation, Climate change mitigation, Energy, Environment and health, Industry, Land use, Marine, Policy instruments, Resource efficiency and waste, Soil, Specific regions, Sustainability transitions, Transport, Water and marine environment Coastal https://data.unep-wcmc.org/ Ecosystem, Biodiversity 9.2 EDS Course Map Here are the suggested courses at Yale and tracks to improve your data science skills. EDS Course Map "],
["references.html", "References", " References "]
]
